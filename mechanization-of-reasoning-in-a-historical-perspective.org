* Mechanization of Reasoning in a Historical Perspective

Started reading:
21 August 21 8:19 pm IST

The idea of formalization of reasoning starts from the middle ages and reached its maturity in modern logic that is in Frege → Peano → Russell and Whitehead → Hilbert → Lukasiewicz → Gödel → Tarski → Jaśkowski → Leśniewski etc.

Elimination of quantifiers results in reducing whole of logic into binary algebra.

Formalization of logic effectively resulted in us pursuing inquiry into the nature of intelligence by providing us a suitable base.

Reasoning stands for deduction in this volume.

A thinking machine is an information-processing system that has a hardware component, the processing being aimed at growth of information; Information processing is found in a continuous interplay with data processing, the former being construed as operating on abstract objects (numbers, propositions, etc.), the latter as operating on physical tokes (e.g., numerals produced with ink, or by magnetization of some spots, etc.) which represent pieces of information.

Logic is the theory which deals with a special kind of information processing, namely that which preserves the truth of information.

Two sense of mechanization:

1/ Mechanization as synonymous with formalization. Formalization as in Post production systems, where you can deduce theorems through syntactic manipulations
2/ Mechanization in the narrow sense that there exists a device and a software to operate on it in order to process sentences (syntactically defined string of tokens) according to an algorithm involved in the given formalization.

Formalization or mechanization in the sense of 2/ consists in using a device and a mapping to this device of an abstract machine.

The work of Shannon in interpreting circuits as performing boolean logic allowed for the method of reducing predicate logic to propositional logic due to Skolem, Hilbert, and others which enabled the use of binary digital circuitry for mechanizing proofs in predicate logic.

One more significant step in this direction is the cut-free formalisms of Herbrand and Gentzen of the predicate-calculus. The cut is a schema representing a number of inference rules, one of them being the familiar modus ponendo ponens, whose use in a proof requires some ingenuity on the side of the reasoner to identify the premises from which the conclusion is being cut must be found.

On the other hand, in a proof produced with the cut-free formalism, each step is determined by the syntactic structure of the formula processed, hence the need for intervention is extremely reduced, and the whole procedure becomes fairly mechanical.

Gentzen demonstrated that any proof involving the cut rule can be transformed into a proof in which the cut does not appear, and this implies availability of a mechanical procedure for any proof formalized in predicate logic.

CU = <Information + Data + Machines?;
       Natural Machine, Artificial Machine, Selves;
       Rec[d,i]
       PrI[i,j]
       PrD[d,e]
       PrT[t1,t2,i],
       Cns[s1,x]>

Rec[d,i]: Piece of data d is assigned the information i.
PrI[i,j]: Transforming information into pieces of information
PrD: Transforming data into data
PrT: Transforming Things into Things (or, a state of a thing into its another state) through a piece of information such as a dog acting to the command of a master.
CnS[s1,x]: Self conscious minds. A mind is capable of being conscious of any object whatever including its own acts. This kind of consciousness is called apperception.

*** 1.3 Information-processing through data processing

Information pieces are entities which are involved in the interaction between the physical world and the world of abstract entities.

What all information pieces have in common is their relation to some physical objects termed as data. The relation involved may be called expressing, representing, articulation,g, formulating, signifying, recording etc.

Verb Recording is considered standard here.

Thus an uttered (spoken or written) sentence is a physical event being a datum to record a proposition. A score, as a sheet of paper covered with notes is a datum to record a piece of music. A drawing is a datum to record the design, say of a house. A configuration of polarized spots on a magnetic medium forms data to record, e.g., a program for computer.

To some extent, the elusiveness of the notion of IP can be remedied by a certain combination of the idea of recording relation with the concept of equivalence (or abstraction class) as defined in logic and set theory.

An equivalence relation is reflexive, symmetric, and transitive. Whenever one has an equivalence relation on a set S, the set can be partitioned into a number of disjoint sets called equivalence classes such that all the members of any one equivalent class bear the relation R to each other but not to any members of S outside that equivalence class.

Cardinal numbers are defined by reference to equivalence classes of equinumerous sets. E.g., number 2 is presented as the set of all pairs. This does not mean that the terms 2 and the class of all pairs denote the same entity. It is certainly true that # > 2 but this does not imply that triples is greater than the class of pairs, hence the respective arguments of the predicate ‘is greater than’ do not prove interchangeable.

The method of introducing abstract objects through equivalence classes, as exemplified above, especially to examine relations between information-processing and data-processing. The idea to be developed is to the effect that pieces of information are abstract entities, in a way assigned to respective equivalence classes of pieces of data.

Author is trying to establish a correspondence between two chains, that of data and that of entities represented by data. The most basic is the correspondence between numeral sequences as data and numbers as entities represented by such sequences. They belong to 2 different domains.
Successor operation turns a number 10 to 11 and in the codomain it turns the numerals ‘10’ into ‘11’. I think it might have been better if the author used the word encode to represent the act of transforming the semantics into the syntax.

The item represented is called an information item and the item used to encode this is called the data.

Data items are recorded in objects such as machines and organisms, and owing to these records information can be stored in objects and processed by them.

Information as the abstract entity and data as the physical entity.

Processing applied to information and data are not independent; they are so related that data-processing is a means of information-processing.

The term ‘sentence’ denote a physical object made from ink, or air waves, or electric impulses, etc. (while the terms ‘proposition’, or ‘statement’, or ‘judgment’ will never appear in this role).

Thus sentences belong to the category of data.

Information pieces recorded in sentences are called propositions, so sentences like 2 + 3 encoded in Roman numerals, Decimal, or Binary representation record the same proposition. Obviously, P is not identical with any of the members of the equivalence class E. Neither with E itself. Were it identical with E, then it would be sensible and true to say, e.g., that P contains the empty class which would be a kind of nonsense.

Then there are information pieces which are abstract entities, each of them being associated with exactly one equivalence class of data.

The notion of blind thinking (caeca cogitatio) was used by Leibniz to indicate the mechanical deduction carried out by computers without a notion of the semantics(?) of say a collection of six objects dividing into two triples.

To sum up the domain of information-processing involves numbers and the domain of data-processing involves numerals. In reasoning, the former consists of propositions, the latter of sentences.

Operation involved in data-processing are formal that is concerned with the form, or structure of strings of characters, that is data, and not with a content. About a reasoning which proceeds solely according to formal rules we say that it is formalized. Should such a reasoning be carried out by a machine, we call it mechanized.

However, there are important difference between these two domains of information-processing which are crucial for our discussion. Data-processing in the sphere of computing, i.e. mechanical calculation on numerals as representing numbers is a usual and unavoidable procedure. But data-processing in the sphere of reasoning, i.e., formalized inference, is a relatively new invention which has a clearly artificial character. Though it has proved necessary for metamathematical research, as well as useful and inspiring for philosophy of mind, it does not prove necessary for efficient reasoning.

In a formalized logical system, we have:

a and b  y infer not y → not a or not base

This is natural language is:

Matter and its motion results in time and space

If matter nd its motion disappeared, there would no longer be any space or time.

** 1.4 Intelligence and model based reasoning

Are there entirely wordless reasonings, such that they do not resort to any piece of a text? If there are, what, then, are the data to be processed? What about the principle that every reasoning is a truth-preserving information-processing, where abstract pieces of information are represented by data as physical entities?

Suppose we say that there are indeed wordless reasonings, what shall we call them? That is the class of reasonings in which information-processing is not supported by any text-processing. Let it be called the class of model-based reasonings. Another designation might be given after Popper in Unended Quest (1982) who observe that we do think in words as well as by means of schemata.

The answer in the affirmative is supported by various kinds of evidence. Due to some natural laws governing organisms, people and animals are capable of forming, e.g., internal pictures of things. Such pictures and similar devices, some of them of more abstract character, can be called models. This term enjoys sufficient generality and, like ‘picture’ involves the notion of similarity in its content.

Why should a craftsman, or an engineer try to name all the states of the engine under repair, when she seems them with her eyes, and can test her unspoken estimations with movements of her fingers? Her reasoning consists in transforming such visual and tactile data in her imagination; seh is not bound to record them in her notebook and adopt predicate calculus for their inferential processing.
In a model based reasoning, the data processed are models, and those are due to records made in a code inside a processing system (e.g., a visual percept encoded in a brain).

These records are also data to represent pieces of information, while in a text-based reasoning information pieces are represented by data formed as sentences. In both cases appear pieces of information of which either truth or falsity can be predicated (e.g, the truth of a percept involved in a model), hence the difference in their representing by verbal data in one case and model data in the other does not affect the nature of reasoning as a truth-preserving transformation.

The point to be vindicated is to the effect that there are reasoning which the reasoner is not aware of, i.e., those which do not occur at the level of conscious reflexion. We shall briefly speak of them that they are not apperceived, taking the term in Leibnizian sense.

Leibniz needed the word perception to denote all acts of living individuals (i.e., substances) as reacting to certain impulses, while fully conscious perceptions were by him distinguished with the specially coined term apperception. He defined it as the reflective knowledge of an inner state, which is not given to all souls, nor at all times to the same soul.

Now the point can be stated briefly that there are unapperceived reasonings in humans and still more briefly there are covert ones. This statement is of great import for the study of intelligence. Provided it is right, the attempts to create artificial intelligence, which would be as close as possible to natural thinking, should not lead toward the text-based mechanical reasonings. Instead, artificial minds should be able to simulate model-based and covert reasonings as proving most efficient in those situations in which the subject of reasoning is not liable to be described in words, and in addition, it has to be grasped in a fraction of the second.

Model-based reasoning is unavoidable in that kind of mental activities which is termed knowing how (in contradistinction to knowing that) by Ryle in The Concept of Mind (1949), and more commonly is called ‘know-how’. This problem was tackled by Herbert Breger in Das Postulat der Explizierbarkeit in der Debatte um die kiinstliche Intelligenz (1988)

There is, actually, no mystery either in model-based or in covert reasonings, they are simply facts of every-day life. Nevertheless, there are philosophical schools such that one of them denies possibility of covert mental acts, and the other possibility of model-based reasonings. From the latter point of view, that of behaviorists, there is a mystery in the conception that some reasonings might be non-verbalized since any thinking is by them construed as an inner silent speaking.

On the other hand, a covert reasoning is regarded as impossibly by the Cartesian philosophy of mind in which the mind is identified with the subject of conscious acts.

According to Descartes, there can be no covert reasonings, as reasoning is the affair of consciousness alone: I reason then and only then, if I know that I reason. There is no necessity for reasonings - meant Descartes - to be recorded in words (hence a model-based reasoning might be admitted), but it is necessary for them to be self-conscious. This is why Descartes denied animals any capability of reasoning; he regarded them as mere automata so unable to make inferences as is unable, say, a clock.

Refer Dipert (1994) - Leibniz’s Discussion of Obscure, Confused, and Inadequate Ideas.

In the Leibnizian perspective reasoning is conceived as a kind of information-processing accomplished through data-processing, while the data are not necessarily linguistic; they may be some records in an organic machine which function as models of those pieces of reality which form the subject-matter of our reasonings.

The main mechanical-intelligence problem can be stated in the terms of processing Encoded Potential Concepts. That there do exist Encoded Potential Concepts in human bodies, is a philosophical hypothesis to motivate a research project.

An instance of EPCs on which attention of scholarly circles was focussed in recent decades is concerned with the notion of linguistic competence as introduced by Noam Chomsky; to start acquiring a language, a human being must have some innate potential notions of language, communication, predication, etc. Even if one does not endorse a materialistic point, it is advantageous to imagine those notions as encoded in our bodies, as a kind of data to be processed, presumably in the central nervous system.

An example of Socrates eliciting the idea of Pythagoras theorem in the boy’s mind is described as an example of potential concept encoded.

Besides grammatical and mathematical encoded potential concepts, there are logico-ontological EPCs which prove indispensable at the start of any language acquisition. Among them are those of a class, individual and of equivalence relation.

These 3 ideas are involved in any act of realizing that an individual object shown in the moment should represent a class to be named so and so, namely the class of those individuals which are identical - in a certain respect with that being produced (i.e., an equivalence class). No communicative act involving ostention would be possible without functioning these logical EPCs, hence they must be innate in every human individual.

A mechanized reasoning with the utmost clearness reveals that any reasoning is a truth-preserving information-processing carried out by means of data-processing, the data being entities as physical in their nature as are electric impulses, or magnetized spots, while the property of truth-preservation is revealed in the explicit application of deductive rules.

The author seems to suggest a program for achieving artificial general intelligence.

One should i) discover mechanisms of model-based reasoning in order to imitate them with artificial devices
ii) furnish such devices with a set of encoded potential concepts similar to that enjoyed by humans
iii) master the process of transforming unconscious, only potential ideas into fully apperceived concepts

* Chapter 2
The Formalization of Arguments in the Middle ages

** 2.1 The contention of the present chapter

The key notions in the story are those of data-processing and information-processing. The former can be entrusted to machines provided that information items (abstract objects) are represented by data (physical objects), hence the results of data processing due ot a machine can be read off by a human as results of information-processing. In a process of reasoning, the formalization consists in recording propositions (information items) as data, and in putting forth inference rules as operating on physical objects.

The notion of Llull as the forerunner of mechanization though no traces of formalization appear in his doctrine as propagated in works on history of logic and history of computer science:

Formale Logik — I. M. Bocheński (1956)
Logic, Machines, and Diagrams — Gardner (1958)

Llull’s candidacy to the status of the principal predecessor of Leibniz and the initiator of the mechanization of arguments proves untenable.

It is true that Leibniz had predecessors in the Middle Ages, but these are rather to be sought in the trend that was in opposition of the Neo-Platonic orientation represented by Llull, namely in the nominalistically-oriented logica modernorum in the late Middle Ages, also called terministic logic because of its semiotic inquiries into the so-called proprietates terminorum.

Risse in his Die Logik der Neuzeit (1964) says:

“The Lull school is to be understood solely through Lull, not by reference to Leibniz. True, Leibniz owes many particular ideas to it but he organizes them in his own way. For with Lullists logic was neither primarily nor essentially connected with mathematics […] (Leibniz’s) calculus ratiocinator rooted in Vieta’s algebra speciosa, is a product of the 17th century, and ars magna played no noticeable role in it”

The opinion that Renaissance mathematicians invented variables must be taken with a grain of salt. It is not groundless to ascribe that idea to Aristotle ast he author of syllogistic schemata. Others, for instance A. N. Whitehead, ascribe the principal merit to Archimedes. When it comes to such a sophisticated concept one may assume in advance that it was developing for centuries stage by stage, and whichever stage is taken to be the turning point in that process, the decision will always be arbitrary.

Dissertation de arte combinatoria in qua, Arithmeticae fundamentis, Complicationum ac Transpositionum Doctrina novis praeceptis exstruitur, et usus ambarum per universum scientarum orbem ostenditur; nova etiam Artis Meditandi seu Logicae Inventionis semina sparguntur. Prefixae est Synopsis totius Traclatus, et additamenti loco Demonstratio existentiae Dei ad Mathematicam certitudinem exacta. - Leibniz (1646)

*** 2.2 Heuristic algorithms in Middle ages

The problem of a certain formalization of arguments was for centuries discussed in terms of the logic of discovery, and hence in terms of processes which we now treat as typically creative and not subject to mechanical procedures.

That was due to a combinatory interpretation of the process of discovery, which involved finitism and formalism. Finitism because effectiveness requires that the number of combinations be finite. Formalism because combinations must be carried out on some discrete object that can be unambiguously identified; these conditions are in a model way satisfied by material symbols owing to their visible shapes (or forms, hence the term ‘formalism’) and discrete arrangement. That finitistic formalism started in the late Middle Ages and culminated in the 17th century.

The position of theory of reasoning in the structure of traditional logic. Tripartite ordering schema ordered by singling out three hierarchically arranged operations of the mind:

Fundamental operation: Grasping things by concepts. Simplex apprehensio. It is the simplest one in the sense that in that part of the act of grasping thing which occurs at the level of consciousness, and so is accessible to introspection, we do not perceive any components that could be clearly isolated.

The second operation of the mind, second in the sense that it assumes the existence of concepts and is more complex than conceptualization, is the formation of judgement. According to Lullistic logic, nad scholastic logic in general, it consists in the combination of concepts into a judgement (judicium). This had suggested to Llullus the idea of a combinatorial procedure of generating judgements.

This is said to be an Utopian plan because in the construction of sentences we can admit predicates with an arbitrary number of arguments; and the imposition of any constraint upon the number of arguments would either be conventional or appeal to intuitions that are far from being mechanizable.

A judgement was treated as an invariable tripartite structure consisting of the subject, the copula, and the predicate, the copula expressing either affirmation ('is') or negation ('is not'). This is how a judgment (protasis) was understood by Aristotle. For a properly limited dictionary of terms (i.e., expressions which can function as either the subject or the predicate) this yields a realistic algorithm to produce the list of all possible judgements. From such a list one would then have to choose those judgements which are true and as such are suitable as premises of those syllogisms that are to yield truth. Today we find it difficult to imagine what algorithm which is not a proof (because the proof, e.g., a syllogism, comes later as the operation that is next to the formation of judgments) could be constructed for that purpose: empirical truths are beyond the reach of any algorithms whatever. The point is, however, that for the representatives of the Platonic-Aristotelian views (which came to be opposed by modern empiricism) truth in the strict sene of the term was identical with necessary truth or (approximately) analytic truth, that is, the glorified logic of discovery, ars inventionis, which preoccupied many outstanding minds in the Middle Ages and the Renaissance until 18th century.

The third operation of the mind, the most complex one in the sense that it assumes the two preceding ones and brings the most complex product, consists in the proof, construed in the Aristotelian logic as a syllogism. This identification of proofs with syllogism in that tradition is essential for the present discussion because syllogistic combinatorics, which is the foundation of mechanization can then be identified with the theory of proof taken as a whole, which would allow one to conclude that all proofs can be mechanized.

By a proof Aristotle means a syllogism that creates scientific knowledge. Hence if knowledge is such as we have stated, then the premises of demonstrative knowledge must be true, primitive, direct, better known (than the conclusion) and must be its cause.

According to Aristotle’s interpretation the difference between a proof and a syllogism is epistemological and not formal logical in character: a proof is a syllogism hose premises meet the epistemological conditions of scientific knowledge (episteme) as distinguished from common belief (doxa). Hence in science there are no proofs other than syllogisms, even though not every syllogism is a proof. Thus syllogistic exhausts the whole formal logical part of the theory of scientific proofs to which, in Aristotle’s intention, his Prior Analytics (concerned with formal logic) and Posterior Analytics (concerned with methodology of science) were dedicated.

Someone who knows the history of Greek and later mathematicians might object the point that logicians of that time identified a proof with a syllogism. They must have known the procedure employed in proofs by Euclid and other mathematicians which hardly resembled syllogistic forms. If so, why did they claim that every proof should be a syllogistic inference?

Nowadays it is well-known fact that syllogistic can be interpreted in the monadic predicate calculus which is a decidable theory. This fact may have been intuitively sensed in practicing syllogistic inferences, and together with identifying the whole of logic with syllogistic that might have led to the belief in the possibility of mechanizing all reasonings. This belief seems to have been favoured by other factors in the cultural context in which logic existed for two millennia. That context involved two philosophical tendencies, namely finitism and formalism.

In the Greek philosophy of mathematics finitism established its place for good owing to the paradoxes of Zeno of Elea (490-430 B.C.) which showed how formidably perplex are the problems resulting from the concept of (actual) infinity. That was reinforced by the authority of Aristotle who in his Physics and Metaphysics advanced arguments, to be later repeated for centuries, against the existence of actually infinite domains. The opinion was also represented in antiquity by other schools (TODO: Research which ones held to this idea) but Aristotle’s voice sounded more loudly, especially when supported by Christian thought: the major part of his representatives reserve infinity for the Creator alone while denying it to the creature. The finitist camp included such influential authors in Christian antiquity as Origen (185 – 254), with whom centuries later Cantor himself would engage in vehement disputes, Proclus of Constantinople (410 – 485), an influential commentator of Euclid, and - in the period of the flourishing of medieval philosophy - Thomas Aquinas (1225 – 1247), the greatest Christian Aristotelian.

That camp did not include Augustine of Hippo (354 – 430), but his standpoint, voiced only in connection with other problems and hence likely to be overlooked, was fully understood probably only by Cantor (who sought in him as ally in polemics with contemporary theologians).

Thus the intellectual climate in which the Lullists were active favoured finitism. The view which implied the finiteness of both the domain of individuals and the set of concepts provided people with reasons to postulate the decidability of the system of human knowledge. True statements would be deducible from a finite set of first principles (as Aristotle claimed), and false ones would be refutable by demonstrating that they contradict those principles.

The ideas of the potential infinity of human cognition, of the limits of verbalization, of the approximative nature of scientific theories, and the like, have become familiar to the modern mind only recently.

As long as it was believed that concepts and judgements had adequate mappings in language, on the one-to-one basis, there were reasons to believe that thoughts could be fully replaced by words, and these, being material objects, could be processed mechanically.

Augustinism opposed Aristotelianism both by its infinitism and its doctrine of illumination, which stressed the intuitive, non-mechanizable elements of cognition; that tendency even more manifested itself in the gnostic movements. But as for the problems with which we are concerned here the essential point is that both the finitistic and the formalistic trend were firmly rooted in the medieval thought.

Inventio medii: The problem of finding the middle term.

It was typical of logica inventionis, that is, the logic of discovery, postulated also by Lull, who wrote, among other things, the treatise entitled Ars inventiva veritatis.

Averroes (1126 – 1198) one of the greatest Arab Aristotelians, was the first medieval author known to have coped with the issue. On the other hand, Albert the Great (1193–1280) was the first who, following Averroes, assimilated that problem to Christian scholasticism. Albert’s another merit consisted in that he took up, after the Arabs, combinatorial investigations concerning the number of all possible syllogistic structures in order to find all possible correct syllogisms. A similar combinatorial approach can be found in hte Jewish logician named Albalag, who was a contemporary of Lullus and like the latter was active in northern Spain. As can be seen, when it comes to the combinatorial approach, Lull was by far not the first among the schoolmen, and if he did not take that problem for instance from Albert the Great, then he must have most probably owed it, as Albert did, to the Arabs.

Dzieje filozofii europejskiej w XV wieku. Tom II: Wiedza - S. Swiezawski (1974)

Among the authors who took up those problems after Albert the Great special mention is usually given to George of Brussels and Thomas Bricot. They belonged tot he nominalistic trend in the 15th century (logica modernorum), which had originated with he great Oxford masters, William of Ockham and Richard Suiseth, whose teachigns transferred to the Continent by Paul of Venice took strong roots at the universities in northern Italy and central and eastern Europe, including Prague, Cracow, and Leipzig.

Thomas Bricot presents a sort of algorithm on how to find the middle term. The full set of such rules in the form of a graphical schema, nicknamed pons asinorum, was given by Petrus Tartaretus in his commentary to Poryphyry’s Isagoge and to Aristotle’s logical writings; the diagram itself is supposed to have been constructed ca. 1480. The term ‘bridge of asses’, to this day preserved, oscilattes between two meanings. Tartaretus himself, in his desire to prevent his students from experiencing anxiety in view of the complexity of the diagram, explained that such anxiety would be as groundless as that which is felt by asses which are to enter a bridge, because that diagram is to ensure a safe passage, and not to render it difficult. In the other meaning, until today to be found in dictionaries, one takes into consideration the feature of facility which marks algorithms, that is, mechanical procedures that do not require inventiveness on the part of the user, and as such are manageable even by proverbial asses.

The picture that emerges from the foregoing overview does not confirm the opinion about Lull’s role in shaping the idea of the mechanization of arguments. The key role was played by late medieval nominalists with their definitions of logic as pertaining to terms, and hence physical objects on which mechanical operations can be performed (note in this connection that the leading nominalists, such as Ockham, Suiseth, and Buridan, were also forerunners of modern mechanics). It was nothing else than terministic logic, that logica modernorum which formed the main link between Aristotle (whose texts admit a “terministic” interpretation but do not forejudge it) and Leibniz, who, after having found himself, together with his Leipzig masters and Thomas Hobbes in the sphere of nominalistic inspiration, transformed it in his vision of logic embedded in a mechanism.

There was vehement polemics with Aristotelian and scholastic logic which took place in the Renaissance and the 17th century. What strikes us today as the value represented by the formalist wing of logic, namely the approch to possible mechanization, was severely criticized, at first by numerous humanists, who postulated that logic should “come closer to life”, the postulate marked by their practical and psychologistic attitude, typical of rhetoric.

At the same time there was criticism in the vein of Francis Bacon, which demanded that logic should discover hard natural facts ad not mere terms (such as the middle term to be found with medieval recipes).

In the 17th century, in turn, the formalistic trend had its main opponents in Descartes and his followers, who ascribed to logic the role of the healer of minds. According to that programme logic was to protect human minds against deviations, including the scholastic and formalistic ones, and to make them capable of finding the truth. In that respect it also deserved the name of logicae inventionis, but in a new sense, namely that of Cartesian rules for the seacrh for the truth with the natural light of reason, which formed a certain linkage to the Platonic/Augustinian tradition with its key concept of illumination.

One could hardly deny the pertinence of Cartesian criticism when it was aimed att he triviality and sterility of such formal systems of rules as pons asinorum. In fact, an acute human mind never makes use of them in its search for the truth. Today we realize even better than the Cartesians did that inventive intuition is what can never be mechanized. Hence, should the only task of logic consist in the guidance or reinforcement of human minds, one could, and even had to, agree with the Cartesian critique of scholastic formalism. Yet it turned out, and that just in our century, that logic can be used to mechanical processing of knowledge.

Bridge of asses refers to the relations of inclusion and mutual exclusion which is present in the extensions of the term involved in the thesaurus of a language with a grammar and a vocabulary. ALl this is, self-evidently, the same as the formulation of a certain axiom system of language, that system playing the role of axiomatic rules in the sense as understood by Ajdukiewicz in Sprache und Sinn (1934). The axioms in that system should be independent of one another, which makes the system desirably economical. Endowed in this way the computer becomes a master of ars inventionis in the sense of scholastic formalists, for it can faultlessly find the middle term required in the proof of a given thesis. It will do that by searching the vocabulary and accepting for a given proof those terms which bear to one another extensional relations required by the rules of the bridge of asses.

The term ‘follows’ in it being interpreted in the sense of set theoretical inclusion. This makes me think of filters in lattice theory, where an ultrafilter stands as evidence for weaker truths above it.

For our discussion it is of minor importance that the formal apparatus of scholastic logic is so limited as compared with the needs of the automatic processing of knowledge. The essential poitn to be seen in the similarity of programmes, which makes us understand the rules of logic as rules of operations on physical objects of a definite form (hence formalism) which at the point of departure constitue a discrete and finite set (finitism). That approach to logic, which may be read between the lines in Aristotle, was consciously taken by medieval formalists, and then developed by Leibniz and (independently of him) by later authors, especially by George Boole; it was Boole likewise Leibniz from whom Frege took the idea of his logical enterprise.

Frege’s 1880/81 manuscript contains a comparison between Leibniz’s, Boole’s and his own approach.

*** 2.3 The role of Lull and Lullism

Ramon or Raymundus Lullus was born in 1232 in the capital of Majorca, an island recovered from the Arabs in 1229 by the Catalan army led by Jacobus I or Aragonia.

Lull’s famous invention discussed in the history of logic was called ars magna by himself. It covers a certain technique of forming judgements and also something which might be compared to an axiomatic system in the field of philosophy and theology.

Most formal part of Lull’s art, which might be termed combinatorial syllogistic, is to be found earlier in Albert the Great; the latter took it over from still earlier Arab authors, and the problem originated with Aristotle who asked about the way of finding the middle term. The fact that Lull’s thoughts were imbued with that methodology can be sufficiently explained by his being versed in Arab logic, which he presented in his earlier writings, such as Logica del Gazzali (the Catalan version of the name of an Arab logician), written by him in Arabic and translated by himself into Latin and Catalan.

The core of the argumentation of Lull in the most concise form is tob e found in the discussion which he had with an Arab dignitary during one of his later missionary travels (the discussion ended in Lull’s imprisonment at the moment when his Arab opponent ran short of arguments).

Lull had earlier agreed with his opponent that their discussion would take as the starting point the common belief in goodness as one of the principal attributes (dignities) of God. The belief that God has all those attributes was part of the common heritage of both Christian and Arab metaphysics of those times, shaped in the Neo-Platonic melitng pot. By departing from that common point Lull argued in his discussion, like in every other one in which he engaged, that in accordance with the Neo-Platonic principle (also common to his opponents) bonum est diffusivum sui, the goodness of God must spread of necessity. If that diffusion did not consist in giving rise to the second and third person of Trinity, then God would have to manifest himself only by the creation of the world, and then the manifestation of his goodness would depend on something which is not necessary, and hence imperfect and thus unworthy of God. In his first speech in Tunisia Lullus expressed that by saying that without the internal activeness within the Trinity God’s dignitates, including his goodness, would be idle and hence would not be as perfect as it becomes the Absolute.

It is pointed out in the book that the reasonings one derives out of the judgement could be both correct and wrong ones as there is no mechanical procedure of verification of certain formations as deriving only the true statements in Lull’s Ars Magna.

By bringing in a combinatorial exhaustion of the space, Lull intended for his machines to allow to reach those combinations which are fundamental and natural, their truth being grasped with “the natural light of the mind” (to use the Cartesian phrase, which is due to the same Augustinian tradition).

In a sense, the procedure like that may be interpreted as the search for a third term. In a debate, say, one denies that human acts are subjected to deterministic causation, hence are not free, and in order to prove his point one resorts to the concept of divine concurring as one that yields the middle term. Thus one argues: “Every act of human will is supported by divine concurring; every act supported by divine concurring is free, hence every act of human will is free.” Note, however, that this kind of pursuing the third term is quite a different thing than that prescribed, e.g., for the pons asinorum procedure. No rules of the formal correctness of an argument are at stake but merely a heuristic device to activate memory, and so to find a concept which otherwise might have remained unnoticed. Once noticed, the concepts in question are perceived as connected in a necessary way, on the basis of an intellectual intuition. This should put an end to the legend of Lull as an author of a program of arguments mechanization.

In Part XIII, the last one, in which the method of using the Art is discussed, we find an advice for the teacher that he should make it plain to his disciples that the Art cannot be used without recourse to its close companions, that is, the reason, the subtlety of intellect, and good intentions. In particular, the reference to good intentions, which is by far not incidental but results from Lull’s mystical attitude, disproves the claim that Lull’s Art, as a theory of argument, was a medieval anticipation of the program of algorithmization or mechanization of proofs. Even if there were such anticipations of formalistic approach to reasoning in Lull’s times, they are to be sought (as shown above) in the nominalist, i.e., terminist trends, and not in his intuitionistic doctrine.

A promising way to gain them starts from reading Leibniz’s dissertation De arte Combinatoria in which Lull’s project appears in a very wide spectre of similar enterprises, all of them grown in the same culture of syllogistic; in that culture the reduction of logic to certain subject-predicate forms created the illusion of the decidability of all problems through exhaustive combinations of subjects and predicates.

Thus the said title proves symptomatic of that philosophy of knowledge, and of the resulting research programmes, which started with Aristotle, and did not entirely disappear until the scientific revolutions of our own century (quanta, relativity, evolutionistic cosmology, Gödel limitative theorems). When taking advantage of the cosmological concept of stationary universe (as opposed to the evolving universe), and linking it with the familiar logical idea of the universe of discourse (as the totality of the things under study i.e., those represented by variables), we can introduce the analogical pair of notions: that of the stationary universe of discourse and that of the evolving universe of discourse.

The combinatorial approach is associated with the vision of the stationary universe of discourse, in which the set of concepts forming our knowledge, and thus constituting the universe of discourse, is definitely fixed and closed. Then the whole cognitive enterprise would consist in searching for those combinations of members of the universe which yield true propositions, while logician’s task would involve providing people with a proper method of doing that. It was just the programme of the famous logica inventionis as mentioned in the subtitle of De arte combinatoria. The combinatorial approach, though, turned out quite sterile as far as the development of knowledge is concerned, since the universe of discourse of scientific theories rapidly expands as ever new concepts appear, and that would require ever new recombinations.

On the other hand, the combinatory strategy is fairly suitable in the problem-solving as carried out by computers. It can be seen in the mechanical checking of a proof, where each formula has to be resolved into its atomic components being combinatorily surveyed in order to detect inconsistencies among them, if there happen to be any.

The author whom Leibniz praised most is Thomas Hobbes to whom he owed the idea that every mental operation consists in computing.

The author divides the components of analysis of Lull’s merit to the claim of being a pioneer to computation into: combinatorial and formalistic. It is said that while formalistic is to be given more significance in assessing Lull’s role (by which Lull hasn’t anything much novel), Lull’s combinatorial aspect should not be entirely disregarded.

Nevertheless, the combinatorial aspect should not be entirely disregarded. It played a crucial role in the erroneous belief in the stationary universe of discourse, i.e., that human knowledge can be definitely and safely established by combinatorial procedures. That human brains were mistaken for computers in those ages in which nobody dreamt of the latter, it was one of those errors which paved the way to new understandings.

* Chapter 3
Leibniz’s Idea of Mechanical Reasoning at the Historical Background

** 3.1 An interaction between logic and mathematics

Since Aristotle up to our times, and specially in the 17th century, logic was addressed to human beings to improve their intellectual performances. Those intentions notwithstanding, the most important final result of that process, which started with the rise of logic, consists in the invention of reasoning machines. That human minds can do without a logical theory, when relying on their inborn logical capabilities alone, is obvious when one observes the history of discoveries and other manifestations of creative thinking. But the mechanical mind’s inference are due to some devices for which a logical theory forms an indispensable foundation.

The possibility of mechanical reasoning depends on the existence of what we call the logical form of a linguistic expression, that is to say, a structure determined by those terms which are relevant to logical validity. When a reasoning is so formulated that its validity can be recognized on the basis of its form alone, we call it a formalized reasoning. It may be called mechanized as well, but in a broader sense, namely that a mechanical procedure, i.e., one that does not appeal to our understanding of the content, is sufficient to judge the validity.

When we take the term ‘mechanized’ in a strict meaning, then formalization is a preparatory step towards mechanization. The latter consists, so to speak, in expressing the logical form in a language of physical states subjected to some causal laws of physics, be it mechanics (as, e.g., in Babbage’s machine), be it the domain of electronics (as in modern machines). Thus hte logical operations is identified with physical processes occurring in a machine, and this, let us repeat, yields the mechanization of reasonings in the strict meaning. This explaining of the role of formalization of reasonings as crucial for their mechanization should make obvious why so great import is attached to the former in the story being told in this volume.

The merit of founding logic goes back to Plato and Aristotle, the former as the one who discovered logical validity, the latter as the discoverer of how that validity depends on logical form.

As for Plato’s contribution, the core of Socratic method of argument, as presented in Plato’s dialogues, consists in discerning between the truth of a statement and its being deduced with a valid inference. In a typical argumental dialogue, Socrates helps his interlocutor to deduce some consequences forme a view which the partner claims to be true. When, nevertheless, the consequence proves falso, the partner is bound to acknowledge the falsity of his initial view; this reveals how the validity of an argument can be independent of the truth of its premises. Such is the constant strategy of Socrates, though no attempt is made to theoretically justify that practice.

To justify it, one should have resorted to the form of the sentences involved in an argument. This was done by Aristotle by introducing letters to represent variable contents (subjects and predicates) while the form, rendered by expressions as ‘every … is’, ’‘is not’, etc. remains constant. Though originally there was no direct translation of that form into physical states of a machine, the later development from syllogistic to Boole’s algebra of classes, and then the transforming of the latter into Frege’s algebra of truth functions, and next the invention of the method (due to Claude Shannon) to represent truth-functions by some states of an electrical device, has led to the nowadays mechanization of reasoning.

In spite of its appearance of mechanical (because of the rotating wheels) Lull’s art was no step toward mechanization of arguments, for it did not involve any thought of mathematical operations. It was Hobbes who perceived an analogy between reasoning and computing, and then it was Leibniz who enthusiastically endorsed the idea and contributed to its accomplishment in an algebraic manner. This is why Leibniz so appreciated the idea of logical form as found in some medieval students of syllogistic, as akin to the form of algebraic calculations.

There was methodological feedback between logic and geometry consisted in the fact that the ideas worked out in logical theory used to pass to the praxis of geometricians, who in turn provided logicians with the best patterns of proofs. Aristotle, when writing his Analytics, drew from the interpretations of geometry that were known to him the pattern of necessary knowledge and reliable inference, while Euclid, when writing his Elements half a century later, availed himself of the methodological concept of common axioms that is, those that are not specify, say, to geometry, but are drawn from some more general theory; note in this connection that the Aristotelian example of such an axiom — “if equals be taken from equals the remainders are equal” – occurs on the list of axioms in the first book of Euclid’s Elements.

When arithmetic and algebra, born in Babylonia, met in the Hellenistic period with Greek geometry and logic, they gave rise to that gigantic ‘tree of knowledge’ of which our civilization is the fruit. Arab scholars made great achievements in the development of algebra, continued by European scholars.

The realization of the breakthrough manifested itself in the birth of the term analytica speciosa to single out mathematics using such notation that one symbol does not correspond to a single object but to a class or species of some numbers. It was Descartes who became the coryphaeus of that analytics when in 1637 he published his Geometry, to which Discours de la méthode was the annex; in this work he gave a synthesis of geometry with algebra or analytics.

Owing to the maximal generality which the notation using letters gave to algebra, Leibniz was in a position to realize that al etter need not refer to a class of numbers, but can refer to any class of objects of any kind. The use of letters was invented for logic already by Aristotle, but only the successes of algebra could give birth to the idea of an algebraic treatment. That was one of the greatest of Leibniz’s ideas concerned with logic. It was partly materialized by himself, but it was first published in print two hundred years later, after the same discovery had been made in he meantime by other authors.

The permanent imprint of algebra upon the mentality of people living in the 17th and 18th century consisted in their experience of  how an appropriate language renders thinking more efficient and signally contributes to the solution of problems. This fact played its role in the development of a movement for improving the whole language of science.

The second discovery of the algebra of logic took place in England in the mid-19th century, and was due to a Pleiad of prominent algebraicians, of whom George Boole rendered the greatest services to logic. He was helped in that respect by the then nascent comprehension of the abstract nature of algebra, which is to say that an algebraic theory does not refer to any specified domain (which was particularly emphasized by George Peacock). Algebra can, find application or, more precisely interpretation, in a class of structurally similar domains that can be described with the means provided by a given algebraic theory. In this way algebra, after having developed from the old science of solving equations, has become the most general theory of structures, that is systems of objects for which certain operations are defined. Such operations are described from the point of view of their properties, e.g., commutativity or its lack or the performability of operations with a neutral element (such as zero for addition in a certain algebra having an interpretation in the arithmetic of natural numbers), and the like.

One of the interpretations of this calculus called Boolean algebra corresponds to that part of logic which is known as the truth-functional calculus. The same algebra has another interpretation in tradition syllogistic, and it was just that interpretation which was so penetratingly anticipated by Leibniz.

Boolean algebra can be interpreted arithmetically, the domain of natural numbers being limited to zero and unity. It can also be interpreted in many other ways, but two interpretations, one in the domain of logical sentences, and the other in that of sets, are fundamental for logic.

In the domain of sentences, multiplication is interpreted as the linking of sentences by conjunction (and); complement is interpreted as the negation of a given sentence: 1 as truth, 0 as falsehood. If a formula consisting of those symbols and variable symbols standing for sentences is always true, that is, if it is true regardless of whether truth or falsehood is assigned to the variables which occur in that formula, then it is a law of logic and belongs to the logical theory termed truth-functional, or sentential, calculus.

When interpreting Boolean algebra in the domain of sets (which traditionally were referred to as extensions of names) we can in a natural way express the four traditional kinds of categorical sentences, which form the building material of syllogisms.

Every A is B: A * not B = 0
No A is B: A * B = 0
Some A is B: A * (B + 0) ? or A * not B notEqual 0

Such a translation of traditional logic into Boolean algebra enables us to activate a strong deductive apparatus of algebra for obtaining economical and elegant proofs of the theorems of traditional logic.

The anticipation of that interpretation, to be found, in Leibniz’s treatise entitled Generates inquisitiones de analyst notinum et veritatum (inquiry into a general theory of concepts and judgements), differs from the modern form mainly by the fact that in place of the symbols notEqual 0 it has the Latin phrase est ens i.e., ‘is an entity’ (or ‘exists’), while the symbols = 0 have the analogue in ‘non est ens’ i.e., ‘is not an entity’ (or ‘does not exist’). These phrases can be interpreted in at least 2 ways: as stating the existence of objects which are the extension of a certain concept, i.e., a certain set (extensional interpretation), or as stating the existence or non-existence of combinations of properties, i.e., the intension of a certain concept (intensional interpretation). Leibniz himself was open to both interpretations; he valued the extensional one as technically effficient, but believed the intensional one to be better because of certain philosophical considerations. The idea of the translation of a sentence consisting of subject, copula, and predicate, such as “every man is intelligent” into an existential sentence of the kind “non-intelligent manhood does not exist”, was of scholastic provenance, to which Leibniz clearly referred. The brilliance of his own idea consisted in noticing an analogy between such sentences and equations of the algebra he had formulated (which included the operation of linking concepts and negating a concept). In those equations on the one side we find such a combination of concepts (one of which may be negated), and on the other, one of the two ‘magnitudes’ expressed by the terms ‘ens’ and ‘non-ens’.


The founders of the algebra of logic, that is G. W. Leibniz and G. Boole, and also A. De Morgan, E. Schröder, C. S. Peirce and others were convinced that the whole of logic can be contained in algebraic calculus. They were also aware of the fact that traditional logic, even after its algebraic reconstruction, lacked the means required to describe relations (for instance, it was impossible to render in its language even such a simple relational sentence as “for every number there is a number greater than it”). That was why the next stage was to consist in adding the algebra of relations to the already existing algebra of sets (i.e., the analogue of the traditional theory). These new researches, carried out mainly be De Morgan, Schröder, and Peirce, gave rise tot he theory of relations, which became an important and indispensable discipline in the border area between logic and set theory, but its language (combined with that of the algebra of sets) did not suffice to express mathematics in its entirety, either.

The 19th century, regardless of the intentions of the various authors, witnessed the need, and at the same time the possibility, of constructing a universal symbolic language of mathematics in which well defined symbols and precise syntactic rules would replace the vocabulary and syntax drawn from natural language.

At the turn of the 19th century, logic entered a new path by providing mathematics with a universal and rigorous symbolic language, which had also far-reaching (though not universal) applications in other disciplines and in everyday discourse. For that to happen the 17th century program of a universal language had to be revived, which did take place owing to the publication for the first time, in 1840 by J. E. Erdmann of some logical writings of Leibniz which formulated the program. There were three founders of contemporary logic, independent of one another when it comes to ideas even though they had intensive contacts with one another: Gottlob Frege, Giuseppe Peano, and Bertrand Russell. All of them referred to Leibniz’s ideas, and two of them believed themselves outright to be the executors of his testament by carrying out his programme of universal language.

Frege in 1879 published a work which presented the whole of contemporary logic (i.e., the sentential calculus and the predicate calculus) under the title Begriffsschrift, meaning conceptual writing, and can in Latin be pointedly rendered by Leibniz’s term ‘characteristica rationis’.

When new logical calculi developed at the turn of the 19th century, namely the predicate calculus (which overcame the limitations of algebra) and the sentential calculus (which may be treated as a certain interpretation of Boolean algebra), new vistas were opened to the logicians. On the one hand, it was now possible to develop and improve the calculi themselves by formulating ever new versions and by constructing other calculi themselves by formulating ever new versions and by constructing other calculi that could be superstructed upon the former (such as modal logics). On the other, since those calculi abounded in problems to be investigated, it was also possible to pose questions about the consistency of each of them, their completeness, the purposes they could serve, the relations among the axioms and concepts of a given system (the problem of independence), and finally the relations among the various systems (the interpretability of one of them in terms of another, the consistency of one of them on the assumption of the consistency of another, etc.). Precisely the same questions can be posed about mathematical theories. Among these, the arithmetic of natural numbers is of particular importance from the logical point of view, because the remaining branches of mathematics can in a way be reduced to arithmetic.

Thus, when at the world congress of mathematicians in 1900, David Hilbert presented the mathematical community the task of proving the consistence of mathematics (in view of the antinomies discovered at that time), it was known that it would suffice to concentrate on the problem of the consistency of the arithmetic of natural numbers using for that purpose the logical apparatus, both that which was already known at that time and that which was still to be created. We owe to Hilbert the term ‘metamathematics’ to denote such studies. They were carried out intensively in the period 1920s – 1930s and brought out astonishing results due to Hilbert’s companions as well as critics such as Herbrand, Gentzen, Turing, Gödel, and Tarski.

As metamathematics developed, it was more and more penetrated by mathematical concepts and methods as indispensable instruments of research. They were in particular drawn from arithmetic (e.g., the use of recursive functions initiated by Gödel), algebra (e.g., Tarski’s algebraic approach to non-classical logics), topology and set theory.

*** 3.2 The Renaissance reformism and intuitionism in logic

The development of all ideas, and hence also that of logic in the 17th century, is immersed in the melting pot of general civilizational development, in the cultural ferment of the period. The century which will be described here brings out with particular clarity the fact that the masterpieces of intellect grow from the roots of tradition but also from a lively dialogue with the milieu that is contemporaneous with their authors. And that milieu means not only congenial individuals, but also the audience consisting of readers, disciples, opponents, snobs etc., in a word, the entire enlightened public.

During the 17th century, the demarcation line between scientists and craftsmen was liquid at that time, which is illustrated by the large number of craftsmen in the London royal Society, the greatest collective authority in science of the period.

There was one more heritage of the Renaissance from which the 17th century benefited, namely Pythagorean and Platonic philosophy, from its earliest beginnings most closely intertwined with mathematics. The Platonic trend was always present in European thought, especially from the time when it was reinforced by the idea of Plotinus and in that new form, called Neo-Platonism by historians, established contacts with young and vigorous Christendom, in both its orthodox version and that marked by the influence of gnosis. A significant example of that synthesis can be seen in the person of Proclus of Constantinople, one of the widest known commentators of Euclid and a Platonizing theologian, to who ideas Kepler used to refer. The first Christian writers and Church fathers were as a rule Neo-Platonians. This applies also to the greatest of them, St. Augustine of Hippo, whose ideas were truly reincarnated in the 17th century in the work of Descartes, Pascal, and the milieu of Port Royal, and in the late 19th century came to the rescue of Georg Cantor, the authority of the theory of infinite sets, in his clash with the philosophy of mathematics that followed the Aristotelian approach to infinity.

Marsilio Ficino, Nicolaus, of Cusa, Leonardo da Vinci, Nicolaus Copernicus, Galileo Galilei, Johannes Kepler all endorsed Platonic vision of the world.

In the 17th century two vast currents, one of them linked to technology and economics and ideologically based on the catchword ‘the kingdom of man’, and the other permeated by Platonic metaphysical speculation. Both influenced logic when postulated that logic should give the human mind an instrument whereby it could arrive at the truth. While they had one and the same goal in view they disagreed as to the method of reaching the truth: the former staked on induction, whereas the latter, preoccupied iwth mathematics in the Platonic manner, postulated a purely deductive method (more geometrico).

The idea of reform logic developed in the 16th century; and the 15 century was still busy with commenting the classics, namely Summulae logicales of Petrus Hispanes (as Pop he was known as John XXI), and later achievements, such as those dating from the 14th century. A special merit for that continuation went to Paul of Venice, whose Logica Parva continued to be taught at many universities for two centuries to come. A certain logical idea of Paul of Venice in his Logica Magna found its way to the writings of Leibniz and came to play a certain role in the development of extensional logic.

The history of the reform of logic begins with Peter Ramus, remembered as a vehement opponent of Aristotle who he blamed for being unnatural in his approach to logic. Nevertheless, he took over a great deal from Aristotle, namely the theory of definitions, the theory of judgment, and the theory of inference. Two things made him differ essentially from Aristotle, but one of them was in the sphere of programmes (in which one can relatively easily become an innovator), and the other has its source in age-old tradition, but not the Aristotelian.

In his programme Ramus included the requirement that logic be adjusted to natural human thinking, without that artificial abstractedness for which he blamed Aristotle. His followers took up those slogans, and for the two centuries that followed we find in the title of numerous logical compendia various psychological terms connected with the conception of logic as the science of living human thinking and not of any abstract entities.

Port Royal logic is the best known titles of this kind. Others are Tschirnhaus’s Medicina mentis sive artis inveniendi praecepta generalia, Christian Thomasius’s Introduction ad Philosophiam Aulicam. The same may be said about the title of the work written by Wolff, a philosopher who at first was an adherent of the Cartesian school in logic, and hence an opponent of syllogistic, but later, under Leibniz’s influence, became an advocate of syllogistic.

The psychologism in logic was terminated(?) only in the late 19th century by the concentrated attack against psychologism in logic from the position of the mathematically-oriented philosophy of logic (G. Frege, E. Husserl, J. Łukasiewickz, and others).

Another novelty relative to Aristotle’s ideas consisted in the division of logic into the science of judgments and inference and the science of making inventions, in which much space was dedicated by Ramus to definitions (the connection between the method of making inventions and the formulation of definitions will be fully seen in Leibniz0. it was not an absolute novelty, because such a division was introduced by the Stoics more than dozen centuries earlier and was transmitted to the Middle Ages by Boethius, but turning it into the principal claim which fitted with the aspirations of the epoch was the work of Ramus.

When it comes to Francis Bacon, not only one half of logic but the whole of it was to consist, in his opinion, in the art of making discoveries which would pave the way for the kingdom of man. Bacon designed a logic of induction, which was totally to replace the existing logic of deduction. The characteristic feature of deduction is that the conclusion does not contribute any information that would not be contained in the premises: it cannot convey more information than the premises do. He was not the only one to attack logic in this way, because a whole chorus of critics, among whom the voice of Descartes sounded loudest, blamed syllogistic (the only theory of deduction universally accepted at that time) for not contributing anything new to our knowledge. But while Descartes and his followers wanted to replace syllogism by another deduction, modelled on the experience of mathematical thinking, whose creative character would not be questionable, Bacon, guided by an empiricist ideology, failed to perceive such an alternative solution. Hence, if logic was directly to serve the expansion of human knowledge, he had to stake everything on the inductive method. But at that infantile stage of the logic of induction people failed to realize that for an increase of information one has to pay with a decrease of certainty, or, to put it more precisely, a lessening of probability which has certainty as its upper limit. This is so because a general law is intended to be a conclusion drawn from observations to infinitely many possible cases, and therefore tells us more than the observations refelected in the premises, which always cover finitely many cases. But it uncertainty which is the price paid for such an increment of knowledge, because while Nature may to a given moment confirm that general conclusion, there are no reasons to be sure that it will confirm it in each subsequent case.

If this is so, then the logic of induction is not any alternative solution to the logic of deduction but can at most be its complement (which, by the way, to this day is still in a stage of planning, rather than in that of achievement).

The greatness which logic was destined to attain (?) and which it did attain in our century, was reached by the old Aristotelian road when it merged with the path along which mathematics was developing.

The problem of how logic should be is not merely a matter of logical theory. It deeply penetrates the philosophical vision of the world and the mind. Bacon’s design for a reform of logic originated from the empiricist conception of cognition which ascribes to the human mind the role of a passive receiver: it is like a screen on which Nature casts the image of itself through the objective of man’s sense organs, and if the mind is active in any respect then only by posing questions to Nature.

Such empiricism is opposed by apriorism, also termed rationalism (in one of the meanings of that word), which states that there are truths which man comes to know before all sensory experience; they are given to him as it were in advance. In that sense, reason (Latin ratio) is independent of experience. According to the rationalists, the essence of cognition consists in the realization by the human mind of those truths, given it a priori, which are general and necessary. Premises drawn from sensory experience and experimental data may be helpful in that process of deduction, but they never suffice to lay the foundations of our knowledge. Hence logic as an instrument of cognition, interpreted in the rationalist sense, must be a logic of deduction which not only serves the presentation and ordering of the truths that are already known but it can also lead one to discoveries and reveal truths that had been unknown earlier.

Deduction consists in transferring to the conclusion, the content, that is information already contained in the premises, hence in the conclusion there can be as much information as there is in the premises but no more. On the contrary, a discovery consists in arriving at new information one did not have previously. This is why (deductive) logic as an instrument for making discoveries may seem unthinkable. But whoever raised such an object against the concept of creative deduction would be guided by a conception of the mind that differed from that which marked the 17th century reformers of logic. The opinion characteristic of those times had its roots in Platonism, which dominated enlightened minds in the 16th and 17th century from Florence to Cambridge (two famous strongholds of Platonism). In the Platonic interpretation, apriorism involves a certain conception of what we would today call subconsciousness. In Plato himself that conception was associated with the theory of anamnesis, which mean cognition as reminiscence of truth originating from the pre-existential phase of the mind. Hence a discovery meant nothing else than bringing to daylight a truth that was formerly concealed. In other words, it could metaphorically be described as the shaping of the truth as it were from a substance which was given earlier, in the similar way  bud and a fruit are shaped from the substance of a given plant.

That theory was linked to, and brought in fuller relief by, the characteristically Platonic conception of the teacher as someone who helps his disciples to bring to light what was in the shade or a penumbra. Socrates as described in Platonic dialogues defined that activity by metaphorically comparing it to obstetric art, and St. Augustine coined the term illuminatio to name that process whereby human thought comes to light. This gave rise to the conception of dialogue as illuminating interactions, and also the conception of dialectic as the theory and art of dialogue which has deduction as its main instrument which can clearly be seen in Plato’s dialogues. Dialectic later came to be termed logic. Thus from the very inception of Platonism logic was to serve the process of causing the truths that are innate to the mind to pass from the state of latency to that of clear presence. Thus it is not new contents that we owe to logical deduction but a new way in which they exist in human minds. And the fundamental question of the new programme of logic was as to what rules are to be used in decoding the truths encoded in our minds.

Cartesian school:
Baruch Spinoza
E. W. von Tschirnhaus
Port Royal School

The four rules of Descartes:

1. Not to accept anything as true before it comes to be known as such with all self-evidence; this is to say, to avoid haste and prejudice and not to cover by one’s judgement anything except for that which presents itself to the mind so clearly and explicitly that there is no reason to doubt it

2. To divide every problem under consideration into as many particles as possible and is required for its better solution

3. To guide one’s thought from the simplest and easiest objects in order to rise, later on, slowly to the cognition of more complex ones; in doing so one has to assume regular connections even among those which do not form a natural series

4. To make every where precise specifications and general reviews so as to be sure that nothing has been omitted

The Discours of Descartes in its first edition appeared as the annex to Analytic Geometry. This work helped Leibniz and Newton to invent mathematical analysis (differential and integral calculus). That gigantic step toward the integration of mathematics aroused such an admiration and such hopes in the contemporaries that the second half of the 17th century was animated by the vision of a future science, termed mathesis universalis, which would cover the whole of knowledge, philosophy included, in the form of a single mathematized theory.

Discourse de la méthode provided a description of the appropriate operations preformed by the human mind that allows for arriving at the self-evident statements needed as premises of proofs. It also contained an exposition of geometry which from the methodological point of view can be seen as the model for the deduction of theorems from initial self-evident facts formulated in axioms.

The problem of finding self-evident premises needed to prove theorems expanded in the Cartesian school and its milieu to a vast set of problems pertaining to the distinction between the deduction of consequences from theorems already known, which was then called the synthetic method, and the search for premises to support a given theorem, called the analytic method.

Quote from Logic of Port Royal

#+BEGIN_QUOTE

Method can be described generally as the art of a good arrangement of a series of thoughts in arguments used either for the discovery of truth, if it is not known, or for its demonstration to others, if it is already known. There are thus two methods. One of them is the method discovering the truth, termed analysis or the method of solution; it might also be termed the method of invention. the other is the method of making the truth which has already been found accessible to others. It is termed synthesis or the method of composition, and it might also be termed the method of exposition.

#+END_QUOTE

The terms method of solution and the method composition which occur in that description were to render a similar distinction existing already in scholastic logic and earlier in Euclid and Pappus.

The theory of the analytic method, that is of the ars inveniendi, on which the efforts of that generation of philosophers were focussed, evolved in two opposite directions, the psychological and the formalistic. The psychological trend, which more and more concentrated on advice pertaining to the attitudes and behaviour of the mind (with the accompanying moralizing tendency) was particularly strongly voiced in the writings of Christian Thomasius and in those o his adherents, grouped in the then 
 recently founded university in Halle. Thomasius, who followed Ramus in that respect, shared with the latter the (so to speak) practical orientation in logic. His disciples and adherents were strongly influenced by Protestant pietism, which at that time had its centre in Halle, and were thus influenced by the sentimentalist movement with its inclinations to irrationalism.
 
 No wonder, therefore, that ultimately the idea of the methodological identity of mathematics and philosophy, and hence the idea of mathesis universalis was abandoned by them. This ends, in the late 17th century, one of the ramifications in the evolution of logic.
 
 But the 17th century also witnessed the emergence of an antipsychological trend that raised the problem of the art of discovery owing to the idea of algorithm. This is associated with the name of Leibniz.
 
 *** Leibniz on the mechanization of arguments
 
 Algorithm as a recipe for the mechanical performance of tasks of a given kind. The concept of logical form has developed from the observation that certain sentences, e.g., in English, are accepted as true solely on the basis of their structure, with disregard of empirical and any other extralinguistic data.
 
 In order to define the concept of logical truth in a general manner, and hence also the concept of logical form, one makes use of the partition of all expressions of a given language into logical terms (and, or, not, if … then, some, every, =, and the like) and extralogical ones.
 
 A sentence is called a logical truth if it remains true after the replacement in it of extralogical expressions by any expressions, drawn from the same grammatical category — with the proviso that such a replacement is made consistently, which is to say that on replacing A by B in one place we do so in all those (and only those) places in which A occurs in that sentence.
 
 Thought: In this fashion, Lambda Calculus rewrite can be thought of as that which preserves the logical form.
 
 Such replaceability is of great significance for the recording of logical truths. Logical truths expressed in a schematic form by replacing the extralogical content with variables are called laws of logic.
 
 The fact that logical truths are truths solely in virtue of their form paves the way to the construction of algorithms of two kinds: those which give us a mechanical method of deciding whether a sentences is a logical truth, and those which give us a mechanical method of deciding whether a sequence of sentences claimed to be a so-called formalized proof is in fact a proof of that kind.
 
 The invention of algorithms which would serve those purposes was one of the great plans of Leibniz who expected that the invention would result in an epoch-making progress of human knowledge. His expectations came true only in our century, and that was accompanied by the discovery of the fact that they can come true only to a certain extent, because it is not possible to algorithmize the whole of mathematics (Gödel’s famous result proving the incompleteness of arithmetic), and hence that it is a fortiori impossible to algorithmize the whole of our knowledge.
 
 The first flash of the intuition concerned with logical form must be seen in indirect reasoning, that is reasoning by reduction to absurdity, which is to be found in the enquiries of early Greek mathematicians, and which is particularly frequent in Plato’s dialogues (where as a rule it is used by Socrates, who treated it as main tool of his dialectic). An indirect proof starts from the assumption that the statement to be proved, say C (for conclusion) is false. If this assumption results in a contradiction, this means that it is false; but if the assumption of the falsity of C is false, then C itself has to be true.

The very fact of making use of such proofs points to the sensing of logical form, because in an indirect proof two desirable properties of our thinking are separated clearly from one another as if in a prism: the factual correctness, that is truth, of the premises and conclusions, and the formal correctness, that is agreement of inferences with the laws of logic.

Aristotle of Stagira is credited for creating the first system of formal logic. He made skillful use of indirect proofs, but what made him the founder of logic was the fact that in formulating the laws of inference he used for that purpose letters in the role of name variables. This is not to say that he already had a clearly shaped concept of logical form, but it is beyond doubt that his epoch-making invention in the sphere of notation guided the thoughts of all his followers toward the idea of that form.

Stoic Logicians were undoubtedly aware of the formal nature of logic obtained as a result of the use of variable symbols. They used numbers as sentential variables and thus created the nucleus of the logical theory which we now call the sentential calculus, while Aristotle’s syllogistic was a calculus of names.

Logicians active in the late Middle Ages made a successive important step forward by linking the logical form to the functioning of those expressions which were termed syncategorematic and analyzed them in detail. The realization of the fact that syncategorematic expressions determine the logical form of a sentence is particularly clear in the case of John Buridan. We find in Buridan a conception of logical form which resembles the contemporary one and includes the distinction between categorematic terms and syncategorematic ones, the latter being those which determine the logical form of a given sentence. He also took into consideration the impact of the categorematic terms upon logical form; for instance, the form of a sentence ‘A is A’ differs from the form ‘A is B’. Such a concept of logical form was taken over from Schoolmen by Leibniz, who turned it into the focal point of his conception of logic. Yet he remained isolated in that respect, perhaps because of the strong antiformalistic and scholastic tendencies which emanated from the milieu of both ramus and Descartes. That concept was rediscovered only in the mid-19th century by Augustus De Morgan, one of the founders of the algebra of logic. Thus Leibniz had to play the historical role of being the last Schoolman and at the same time the first logician of our epoch.

Cartesian methodology called for a direct contact of the mind with the object of cognition. Leibniz’s methodology pointed to the indispensability of another way of thinking, in which the contact of the mind with the object of cognition is not direct, but takes place through the intermediary of signs as those instruments of thinking which represent the object assigned to them. One of the ways of using signs as instruments consists in our grasping with our thought the sign instead of the object (its designatum).

That intermediate way of thinking about things, in which the thing itself is not present in the sphere of our attention, was called blind thinking (caeca cogitatio) by Leibniz. Operations on large numbers are its simple example. With large numbers, we are deprived of our visual contact or models and have to rely on sequences of figures which represent them. Such sequences are physical objects assigned to abstract objects, namely numbers, and operations on figures are unambiguously assigned to the corresponding operations on numbers.

If we add to cecae cogitatio one element more, then we obtain the concept of algorithm. The term ‘algorithm’ itself was not used by Leibniz, that role was played by the term calculus or by metaphorical expressions like caeca cogitatio and filum cogitationis (thread of thinking). The latter term referred to Ariadne’s thread, that is to the tool which enabled Theseus to move about int he Labyrinth and to find his way out, that is to solve his problem without any thinking, and without the knowledge of the Labyrinth itself.

Algorithm: a procedure consisting of a series of steps such that every step unambiguously determines the next one in accordance with precisely defined rules. Those rules determine the successive steps in the procedure under consideration by referring solely to physical properties (such as size and shape) of objects, e.g., the shape of figures, and the positions they occupy. On joining the self-evident condition that the number of the steps must be finite we obtain the full description of algorithm, which can be summed up as follows:

An algorithm is a recipe for a procedure pertaining to a definite class of tasks (e.g., addition of sequence of figures), which in each case guarantees the obtaining of the correct result after the performance of a finite number of operations, and that owing to the fact that the objects of such operations (e.g., sequence of figures) are reliably identifiable owing to their perceivable physical properties (e.g., shape, position).

In that melting pot of ideas which was Leibniz’s mind, the idea of mechanical computations merged with his looking at logical form through the prism of algebra as it was known at that time, and also with two other ideas, which he received from the past but transformed and combined in his own way. One of them, propagated by Joachim Jungius of Hamburg, consisted in the opinion that the essence of thinking consists in the analysis of concepts, that is in decomposing compound concepts into simpler ones, until one arrives at the simplest ones possible. Those simple and non-decomposable elements form, as Leibniz used to say, the alphabet of human thoughts. He saw the model of such a procedure in the decomposition of a number into factors which yields a product of prime numbers. Another idea, very popular in the 17th century, was that of a universal ideographic language,  whose single symbols would be assigned not sounds but, as in Chinese, directly to concepts. That planned language was then called philosophical language, conceptual script, universal script etc.

Gregor Delgarno - Ars signorum
John Wilkins - An Essay Toward a Real Character and a Philosophical Language

Logical Form + Algorithm + Algebra + Alphabet of Human Thought + Numerical factorization + Universal Language

According to Leibniz, this conceptual language should be constructed in compliance with Jungius’s idea, which is to say that simple symbols, elements of the alphabet, should represent the simplest non-decomposable concepts, while combinations of symbols, analogically to algebraic operations, should express appropriate compound concepts. The rules pertaining to such operations on symbols were to form a logical calculus that would be algorithmic in nature and would guarantee error-free reasoning.

It is said by Leibniz in a letter to Oldenburg dated October 28, 1675 that the truth would be delivered in a visible manner and irrefutable owing to an appropriate mechanism. His similar idea was that one could easily avoid errors in reasoning if theorems were given in a physically tangible manner, so that one could reason as easily as one counts.

Leibniz’s ideas deserve to be extensively commented. However, the problem of their historical influence is a matter that must be handled in a sophisticated manner. Only his programme for mathematical logic and his idea of the universal formalized language were known to posteriority, while his results themselves were not discovered until the end of the 19th century.

This is a precarious position, indeed. The assessment of Leibniz’s position therefore requires a sophistication which does not attribute too much to him, and at the same time is able to acknowledge the great role he nevertheless played — as a symbol of epoch-making ideas and and of the civilizational turn.

Descartes’ and Pascal’s preoccupation with the mind’s inner life counterbalances Leibniz’s tendency toward replacing mental acts by operations on symbols which could be formalized and mechanized. In the Cartesian-Pascalian perspective we see the mind as dealing with objects and relations which are so numerous, subtle and involved that in many cases a verbalization or symbolization must prove inadequate.

* Chapter 4
Between Leibniz and Boole: Towards the Algebraization of Logic

** 4.1 Preliminary remarks

The algebraization of logic initiated by Leibniz marked the decisive step towards the mechanization of argument.s Equally important was the introduction of variables combined with the conception of logical consequence as dependent solely on the logical form as, in turn, dependent solely on logical constants. But that was not so much a single step as the age-long process from Aristotle to the late scholasticism. The next turning point after algebraization is to be seen in the invention by Frege and later Russell of the calculus of quantifiers, which turned logic into a universal instrument of reasoning, yet at the price of what if we follow Hilbert who defined the axioms of that calculus as transfinite might be termed the infinitism of logic. In order to return to algebraic equations which offered the possibility of mechanical combinatorics performable by the computer it was necessary to find methods of eliminating the quantifiers, that is to do something which might in turn be termed the finitization of first order logic. As is known, that was achieved owing to Skolem, Gentzen, Herbrand, and Beth.

Thus the mechanization of logic proceeded in the three-stages development consisting of formalization, algebraization, and infinitization (intended to increase the strength of proofs) combined with the possibility of reducing the latter to a finitized form that can be processed mechanically.

*** 4.2 Leibniz’s direct successors

Bernoulli brothers.
For Bernoulli algebra was superior to logic

Christian Wolff: Abandoned the Cartesian view of logic under Leibniz’s influence. Cartesian view was marked by a negative assessment of all formalism, especially the syllogistic one, and postulated the pursuit of logic as a normative theory of controlling the mind, with the disregard of the role of language.

After turning to Leibniz’s side, Wolff became an ardent adherent of syllogistic and the programme of pursuing it in the algebraic way as operations on symbols alone, whose meanings might be entirely disregarded. Wolff himself did not contribute to the implementation of that programme of algebraization of logic, but in view of the scope and strength of his influence one may assume that his agitation helped to consolidate that state of consciousness which ultimately brought the perfect rendering of logic in algebraic terms.

Johann Lambert, who rendered the greatest services to the algebraization of logic in the 18th century, worked under Wolff’s influence.

Gottfried Plocquet, a German author of French origin, was busy in developing the Leibnizian ideas of universal language and logical calculus. He studied diligently the then accessible logical texts of Leibniz, including Difficultates quaedam logicae. Ploucquet as the author of many works written in German and engaged in public discussions with Lambert, was well known to 19th century German philosophers. He was quoted, although with disapproval, by Hegel, which contributed to his being known in the next century.

The fact that syllogistic constitutes a small fragment of logic and that it was erroneously identified with logic as a whole was not an obstacle to the effectiveness of such a search. That fragment was well suited as the experimental field for the algorithmization of proofs, and further the fact that the medieval endeavors were naïve and little successful was due to the inability of the early authors to combine them with the calculatory approach. But two things were essential: the formulation of the problem and the proper establishing of the area of research. When in the 17th century algebra provided the paradigm that could be used in logic, a new and promising field of research was opened. It is in that field that we find Ploucquet as its typical representative.

Ploucquet tested the efficiency of his algebraic symbolism on the language of syllogistic. For the recording of statements forming the so-called square of opposition he adopted a new syntax, in which he succeeded symbolically to express both a kind of the quantification of both the subject and the predicate, which made it possible to render the structure of a categorical sentence by juxtaposition alone as in algebra, where juxtaposition happen to be a symbol of some operation. Thus by drawing from the traditional language the symbols S and P to denote, respectively, the subject and the predicate he makes a distinction between any of those terms being taken in its whole extension and its being taken in part of its extension, the distinction being indicated, respectively, by a capital and a lower-case letter. The distinction between distributed and non-distributed terms was a scholastic invention on the path to the algorithmization of syllogistic (admired by Leibniz) and hence we have here to do with the scholastic idea in a new symbolic attire.

Ploucquet devised a calculus where the middle term taken universally is placed first and the other is connected to it so that the middle term is placed in the middle.

Men: m
Living Beings: P
S: Aristotle
All men are living beings
Mp, Sm

When the middle term is universaloly taken twice, the order of its position is indifferent.
When m is subsumed under M, if PM is true, then pm is true too.
Hence Mp, Sm becomes pmS, when m is deleted, we have pS or Sp, which is to say that all S is some P or all S is P. This text, deliberately setting up a syllogistic calculus, should be interpreted in the light of the explanation given several pages earlier in Ploucquet’s work and pertaining to the identity of the extension of subjects and predicates in affirmative sentences. Such an interpretation of the structure of the categorical affirmative sentence proves inseparable from the quanification of predicate.

Since in every affirmation the identity of the subject with the predicate is asserted, it is obvious that if the subject is denoted by S and the predicate by p, then p can be substituted for S, and reciprocally, S for p. Thus Sp is identical with pS.

It can be seen from this text that we, as it is customary today, interpret Sp in the quantification calculus as the implication

forAll(x, S(x) => p(x)), and not as the equivalence, forAll(x, S(x) <=> p(x))
that would be at variance with the reversibility (reciprocality) stated in the text quoted above. This is a warning against the interpretation of traditional logic in terms of the quantification calculus with disregard of the context of the old traditional theory. This is so because in those theories which use the algorithm of the recognition of the correct conclusion, based on the concept of distribution of terms, the predicate in a general affirmative sentence has as its extension a certain subset o the set denoted by that predicate in other contexts (or outside of context) precisely that subset which coincides with the extension of the subject.

Now we can follow the reasoning having in view the fact that with our author juxtaposition is a syntactical indicator of affirmative sentences, whereas for negative ones he used his own symbol of the relation of exclusion.

This is reasoning for testing the correctness of syllogism, which is an algorithm that refers solely to syntactic features, that is the shape and position of symbols, as arithmetical algorithms do. We have to do with the following rules: 
1) It is allowed to transpose letters
2) It is allowed to replace a capital letter by a lowercase one
3) Given a sequence of pairs of letters such that the consequent in every pair is identical with the antecedent of the next one. It is allowed to accept the sequence of letters consisting of all antecedents and the last consequent, for instance to pass from ab, be to abc (to be read: a is b and c, for instance from “the lion is a cat” and “the cat is predator” we pass to “the lion is a predatory cat”.
4) In such a sequence of letters it is allowed to omit any letter except for the first and last one.

After such a reconstruction of Ploucquet’s procedure it can better be understood why he considered it as a procedure analogical to the calculus carried out on symbols. Its discouraging intricacy largely explains why, even though Ploucquet’s works were popular in his times, he did not become the Boole of the 18th century.

Ploucquet is an important witness of his epoch in which syllogistic played an important role as the experimental field for the endeavours to transform logic into an algebraic calculus.

The same trend was embraced by Georg Jonathan Holland, an adherent of Ploucquet in the latter’s polemic with Lambert.

** 4.3 The work of J. H. Lambert
Lambert’s works on logic fill at least seven volumes.

Lambert’s work Logische und Philosophischen Abhandlungen (1782) had an editorial introduction written by Christoph Heinrich Müller, Lambert’s compatriot and friend. It mentions a certain conception of the mechanization of arguments. That conception must have been sufficiently interesting and comprehensible to the learned readers of that period, which is borne out by the fact that such an experienced editor as Müller, who was busy publishing German literature from the period 1050 – 1500, commented on it in such a way as the problem were something being a vogue.

Lambert was directly influenced above all by Locke and Wolff.

We are here to do with a case in which the intersection, at a certain point of time, of two mutually independent processes opens a new stage in history. Such intersection may also appear in the fact that a certain group of scholars is engaged with similar intensity in both processes. This was just in the case under consideration: people strongly rooted in the tradition of scholastic logic came to be interested in algebra, and some of them proved creative in that newly born discipline called then logistic speciosa universalis, that is (in a free translation), the general theory of calculating with variables (i.e., sign denoting species of objects, e.g. numbers, instead of individual objects).

Two ideas were drawn from the logic of Aristotle and the schoolmen. Each of them entered into a natural relationship with a certain algebraic concept in such a way that all components came to form a coherent whole. One of the ideas was that of finding the middle term as a means of constructing the proof. The problem consists in the fact how that still unknown middle term is to be found on the basis of what is known, namely the external terms and the intended conclusion. This task was generalized as the problem of finding new data of a definite kind on the basis of given data of a given kind. The logic conceived as the theory of such a search was termed logica inventionis which can be rendered as inventive logic or (more freely) logic of discovery.

Thus generalized programme of inventive logic was characteristic of the Renaissance rather than of the Middle Ages, and had its standard-bearer in Francis Bacon, but the point of departure should be seen in the Middle Ages; a story related to that problem can be traced in Leibniz’s dissertation De arte combinatoria. Moreover, many scholars who combined interests in logic and algebra must have then been impressed by the analogy between such a finding of new data and the finding of the unknown in an algebraic equation.

But if use was to be made of that analogy, the sentences considered in logic had to have a form analogical to that of equations. That would also result in the attractive possibility of constructing proofs by replacing one side of an equation by an expression equivalent to it (this was how the nature of the proof was conceived by Leibniz). But it is well known that none of the four types of categorical sentences of Aristotelian logic possesses the form of an equation.

The theory of suppositions made medieval logicians used to thinking that what determines the logical function of an expression is not only its external form and the related lexical meanings, but also its context, and the intention of the speaker marked in some way. Hence, according to the context in question, an expression one and the same in its form could stand for an individual, either definite or indefinite; Latin did not make any formal distinction in that respect as it lacked articles; in another context a name could refer to a class of individuals, to a universal, etc. Upon that habit of thinking there was imposed the theory of the distribution of terms in categorical sentences. Taking a general term on one occasion in its full extension and on another occasion only in a part of the extension would amount to different suppositions. The latter of the two, that is the use of a term as denoting a subset (of the set which is the extension of a given term in its lexical meaning) can be indicated by a separate word (for instance aliquis i.e., ‘some’) or by the position in the context of a definite sentential structure, for instance, the position of the predicate in a general affirmative sentence (customarily denoted by SaP).

There was the last step to be made. In contemporary logic, when traditional logic is interpreted in terms of the predicate calculus, SaP is treated as a record of an inclusion because both terms in the sentences are taken in their full extension. But if P is taken in a part of its extension, namely that which coincides with the extension of the subject, then SaP is expressing an equation which should be interpreted as =. In this way, we arrive at a form comparable with the form of an algebraic logic. It is worth noting that for the logicians who lived in the 17th and 18th century that interpretation was so obvious they did not think it necessary to explain the matter to the reader, which they would have to do if they addressed their texts to the 20th century readers (who have forgotten the old teachings on suppositions and the distribution of the syllogistic terms).

Lambert says:
#+BEGIN_QUOTE
Logical analysis is an art of deducing unknown or sought concepts from known or given ones by means of identities. Since nothing can be found out of nothing, concepts have to be given to allow one to find the unknowns … General analysis (analytica logica speciosa, logistica speciosa universalis) is an art of deducing concepts from general and indefinite ones. Since the concepts needed for that purpose are indefinite, one cannot use words as these denote definite concept. The most appropriate thing is, therefore, to use letters and other signs as it is also the case in algebra.
#+END_QUOTE

An example:

A certain technical solution which makes it possible to impart to a sentences of the type SaP the form of identity:
S = xP,

where juxtaposition is the logical equivalent of the algebraic operation of multiplication, and the symbol x stands for an object which, when multiplied by P yields an object identical with V. For instance, the sentence, “Every Greek is intelligent” is interpreted as:

The set of Greeks is identical with a certain subset of intelligent beings. That subset is obtained by the intersection of the set of intelligent beings with some other set, in the above formula represented by j, that is, according to Lambert’s expression, an indefinite coefficient. Thus, the syllogism Barbara is reconstructed as the following set of identitieus:

M = xP, s = yM

On replacing M in the second premise by xP, we arrive at the conclusion:
S = yxP

Leibniz als Identifikationsfigure der Britischen Logiker des 19. Jahrhunderts - Volker Peckhaus (1994)

The research of Volker Peckhaus is to the effect that Boole and his British colleagues first discovered the algebra of logic, and then learned that it has been alreday discovered by Leibniz. This is not to mean that the same was made twice; in Leibiz one finds just some primordial ideas. Anyway, the consciousness of anticipating those achievements by Leibniz proved a significant circumstance in the further development. The problem of how much modern logic, especialyl, algebra of classes was anticipated by Leibniz belongs to main issues in Leibniz scholarship. In this book a rather conservative attitude is assumed, for bolder interpretations require more research. Among those who pioneer such a bolder approach there is Wolfgang Lenzen.

The statement of that sigificance may seem strange, since the lack of any causal nexus between a forerunner and a genuine originator, in the case of Leibniz and Boole confirmed by most recent investigations sohuld imply that the history would have developed in the identical way even if the forerunner had never existed. However, this is not the case in the ihstory in question, which can be explained by a kind of feedback between own results and their later detecte anticipation. Boole and other British logicians, including Jevons (specially important because of the step he made from algebraization to mechanization) not only produced technical results but also entertained some philosophical ideas which did not conform to the empiristic trend, characteristic of the British philosophy. It required a strong belief that the laws of thought were independent from empirical reality, in order to overcame the pression of the British empiristic orthodoxy; that belief gained a new vigour, due to its meeting with Leibniz’s tenets, in the period in which it must have fought for recognition and standing in the British philosophy (had not such a recognition become the case, there would not have been the way paved to Russell’s (1900) seminal work on Leibniz).

For instance, William Stanley Jevons contrary to the teachigs of John Stuart Mill, William Whewell, etc., tried to deductively establish the scientific method on the basis of Boole’s logical principles. As for Boole himself, he voiced his conviction that the laws of logic have a real existence as laws of the human mind, independent from any observable facts (contrary to the creed of the British empiricism). As commented by Peckhaus, such statements of Boole prove his affinity with Leibniz. This may be taken as a proof of reality of what in German is called Geistesgeschichte, that is the history of objective ideas which must come into being, due to their internal logic, in a way which is relatively independent from biographical circumstances.

* Chapter Five
The English Algebra of Logic in the 19th Century

The idea of the mathematization of logic and the development of the formal algebra in the 19th century were sources of the algebra of logic established by De Morgan, Boole, and Jevons. It was in fact the beginning of the mathematical logic. The old idea of a logical calculus which would enable the analysis of logical reasoning with the help of a procedure similar to the procedure of solving equations in algebra was realized.

** 5.1 De Morgan’s Syllogistic and the Theory of Relations

In traditional logic, we have statements of the form:

SaP
SiP
SeP
SoP

called the categorical sentences.

Aristotle observed that one can build valid schemas of inference consisting of two premises and a conclusion being categorical sentence - they are called categorical syllogisms. If we assume that every term in a syllogism stands for a non-emptyc lass then we get that 24 out of 256 possible combinations are valid inferences.

Attempts were made by Francis Bacon, Christoph von Sigwart, and Wilhelm Schuppe.

Quantification of Predicate

William Hamilton noticed that the predicate term in each of Aristotle’s four basic assertions SaP, SiP, SeP, SoP is ambiguous in the sense that it does not tell us whether we are concerned with all or part of the predicate. Hence one should increase the precision of those four statements by quantifying their predicates. In this way we get 8 assertions instead of Aristotle’s four:

all S is all P
all S is some P
no S is all P
no S is some P
some S is all P
some S is some P
some S is not all P
some S is not some P

Using those 8 basic propositions we can combine them to form 512 possible moods of which 108 prove to be valid. The usage of statements with quantified predicates allows us higher precision than it was possible before. For example, the old logic would treat “All men are mortal” and “All men are featherless bipeds” as identical in form; whereas in the new system we see at once that the first statement is an example of “All S is some P” and the second is an example of “All S is all P”. But there arose some problems. It was difficult to express those new statements with quantified predicates in a common speech. Without developing a really complete and precise system of notation one finds himself forced to employ words in a clumsy and barbarous way. Hamilton was aware of it and attempted to remedy the obscurity of phrasing by devising a curious system of notation. Though it was really curious and rather useless in practice it was important for two reasons: it had the superficial appearance of a diagram and it led Hamilton to the idea that by transforming the phrasing of any valid syllogism with quantified predicates it may be expressed in statements of equality. The latter suggested that logical statements might be reduced to something analogous to algebraic equations and so gave encouragement to those who were seeking a suitable algebraic notation. Some logicians such as Howard Gardner in his Logic, Machines, and Diagrams (1958) are of the opinion that this was Hamilton’s only significant contribution to logic.

The idea of quantification of predicate in the syllogism can be found also in papers of another English logician Augustus De Morgan. He was a mathematician and worked not only in logic but also in algebra and analysis. It was De Morgan who introduced the notion “mathematical induction” which was popularized later thanks to the book on algebra written by Isaac Todhunter.

Augustus De Morgan’s earlier logical works written before 1859 were devoted to the study of the syllogism. He introduced independently a system more elaborated than Hamilton’s one. Hamilton accused him of plagiarism and for many years the two men argued with each other in books and magazine articles. This debate had also serious consequences. Namely, it caused George Boole’s renewed interest in logic which led him to write the book Mathematical Analysis of Logic published in 1847.

Trying to reshape and to enlarge the Aristotelian syllogism (cf. the book Formal Logic, or the Calculus of Inference Necessary and Probable of 28147) De Morgan observed that in almost all languages there are so called positive and negative terms. Even if in a language there are no special words indicating this dichotomy, nevertheless every notion divides the universe of discourse into two parts: elements having properties indicated by the given term and those which do not have those properties. Hence if X denotes a certain class of objects then all elements of the universe which are not X can be described as not-X. The latter is denoted by De Morgan by x. In this way the difference between positive and negative terms disappears and they are possessing equal rights. This enable De Morgan to consider instead of two terms of the traditional syllogistic X, Y, four pairs of terms: X, Y; x, y, X, y; x, Y which give 16 logical combinations, 8 of which are different. He introduced various types of notations for them. The first two consisted of letters and resembled the traditional notation: a, i, e, o; the latter two consisted of systems of parentheses.

all X is all Y | A: X ) Y | X))Y
no x is all Y | E: X.Y or X)y | X).(Y
some X is Y | I: XY | X()Y 
some X is no y | O: X : Y or X)y | X(.(Y or X()y
no x is no y | a: x)y or Y)X | x))y or X((Y
no x is no y | e: x)Y or x.y | x )) Y or x(.)Y
some no x is no y | i: xy | x()y or X)(Y
some no x is all Y | o: xy or x : y or Y : X | x()Y or x).)y

We see that De Morgan’s symbols introduced to express the old and new types of syllogisms were not algebraic. But he found that his symbols can be manipulated in a way that resembled the familiar method of manipulation of algebraic formulae.

He established certain connection between the above statements which he called simple. The sign = was used by De Morgan in two meanings. It meant both “equivalent” and “implies”.

Having introduced some fundamental relations De Morgan developed his theory of syllogism. He considered not only syllogism with simple premises but also with complex ones (which he called complex syllogism).

After having introduced operations which we call today the negation of a product and sum he established the following connections:

negation of PQ is p * q
negation of P * Q is pq

He noted also the law of distributivity:
(P * Q)(R * S) = PR * PS * QR * QS

Here we see that developing the theory of syllogism, De Morgan came to the idea of what we call today Boolean algebra. He did it independently of George Boole.

De Morgan not only extended the traditional syllogistic but introduced also new types of syllogisms. In Budget of Paradoxes (1872) he summarized his work under six heads, each propounding a new types of syllogism: relative, undecided, exemplar, numerical, onzymatic and transposed.

De Morgan’s most lasting contribution to logic is when he moved beyond the syllogism to investigate the theory of syllogism in “On the syllogism no IV and on logic in general” from 1859. His ideas and conceptions were later on developed by various logicians, first of all by Charles Sanders Peirce, Ernst Schröder, Giuseppe Peano, Georg Cantor, Gottlob Frege, and Bertrand Russell. Charles Peirce wrote that De Morgan was “one of the best logicians of all time and undoubtedly a father of the logic of relatives”.

In the work “On the syllogism no IV”, De Morgan noted that the doctrine of syllogism, which he had discussed in his Formal Logic (1847) and in earlier papers was only a special case in the theory of the composition of relations. Hence he went to a more general treatment of the subject. He stated that the canons of syllogistic reasoning were in effect a statement of the symmetrical (De Morgan called it convertible) and transitive character of the relation of identity. He suggested symbols for the converse and the contradictory (he said “contrary”) of a relative and for three different ways in which a pair of relatives may be combined (by “relative” he meant what some logicians had called a relative term, i.e. a term which applies to something only in respect of its being related to something else).

De Morgan showed that the converse of the contradictory and the contradictory of the converse are identical. He set out in a table the converse, the contradictory and the converse of the contradictory of each of the three combinations and proved that the converse or the contradictory or the converse of the contradictory of each such combination is itself a combination of one of the kinds discussed.

Papers of De Morgan, though containing new and interesting ideas were not easy to read. They were full of ambiguities and technical details which made them difficult to study. Nevertheless they contributed in a significant way to the development of mathematical logic.

*** 5.2 George Boole and his algebra of logic

Boole was interested in logic already in his teens, working as an usher in a private school at Lincoln and educating himself by extensive reading. From that time came his idea that algebraic formulae might be used to express logical relations.

Boole and De Morgan were in correspondence over a long period (1842 – 1864). They exchanged scientific ideas and discussed various problems of logic and mathematics.

The main logical works of George Boole are two books:

- The Mathematical Analysis of Logic, Being An Essay Toward A Calculus of Deductive Reasoning (1847)
- An investigation of The Laws of Thought, on which are founded the Mathematical Theories of Logic and Probabilities (1854)

The second book of George Boole, An investigation of the Laws of Thought was not very original. It was the result of Boole’s studies of works of philosophers on the foundations of logic. The chief novelty of the book was the application of his ideas to the calculus of probabilities. There was no important change on the formal side in comparison with Mathematical Analysis of Logic.

We should also mention here also his paper “On the calculus of logic” (1848) which contained a short account of his ideas from the pamphlet from 1847. It may be supposed that it was more widely read among mathematicians than Mathematical Analysis of Logic.

Boole was not satisfied with the exposition of his ideas in his books. He was working towards the end of his life on the new edition of the Laws of Thought preparing various improvements. From the drafts published in Studies in Logic and Probability (1952) it follows that what he had in mind was a development of his epistemological views rather than any alteration of the formal side of his work. He mentions in particular  distinction between the logic of class (i.e. his calculus of logic) and a higher, more comprehensive, logic that cannot be reduced to a calculus but may be said to be “the Philosophy of all thought which is expressible in signs, whatever the object of that thought”.
