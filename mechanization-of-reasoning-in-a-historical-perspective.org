* Mechanization of Reasoning in a Historical Perspective

Started reading:
21 August 21 8:19 pm IST

The idea of formalization of reasoning starts from the middle ages and reached its maturity in modern logic that is in Frege → Peano → Russell and Whitehead → Hilbert → Lukasiewicz → Gödel → Tarski → Jaśkowski → Leśniewski etc.

Elimination of quantifiers results in reducing whole of logic into binary algebra.

Formalization of logic effectively resulted in us pursuing inquiry into the nature of intelligence by providing us a suitable base.

Reasoning stands for deduction in this volume.

A thinking machine is an information-processing system that has a hardware component, the processing being aimed at growth of information; Information processing is found in a continuous interplay with data processing, the former being construed as operating on abstract objects (numbers, propositions, etc.), the latter as operating on physical tokes (e.g., numerals produced with ink, or by magnetization of some spots, etc.) which represent pieces of information.

Logic is the theory which deals with a special kind of information processing, namely that which preserves the truth of information.

Two sense of mechanization:

1/ Mechanization as synonymous with formalization. Formalization as in Post production systems, where you can deduce theorems through syntactic manipulations
2/ Mechanization in the narrow sense that there exists a device and a software to operate on it in order to process sentences (syntactically defined string of tokens) according to an algorithm involved in the given formalization.

Formalization or mechanization in the sense of 2/ consists in using a device and a mapping to this device of an abstract machine.

The work of Shannon in interpreting circuits as performing boolean logic allowed for the method of reducing predicate logic to propositional logic due to Skolem, Hilbert, and others which enabled the use of binary digital circuitry for mechanizing proofs in predicate logic.

One more significant step in this direction is the cut-free formalisms of Herbrand and Gentzen of the predicate-calculus. The cut is a schema representing a number of inference rules, one of them being the familiar modus ponendo ponens, whose use in a proof requires some ingenuity on the side of the reasoner to identify the premises from which the conclusion is being cut must be found.

On the other hand, in a proof produced with the cut-free formalism, each step is determined by the syntactic structure of the formula processed, hence the need for intervention is extremely reduced, and the whole procedure becomes fairly mechanical.

Gentzen demonstrated that any proof involving the cut rule can be transformed into a proof in which the cut does not appear, and this implies availability of a mechanical procedure for any proof formalized in predicate logic.

CU = <Information + Data + Machines?;
       Natural Machine, Artificial Machine, Selves;
       Rec[d,i]
       PrI[i,j]
       PrD[d,e]
       PrT[t1,t2,i],
       Cns[s1,x]>


Rec[d,i]: Piece of data d is assigned the information i.
PrI[i,j]: Transforming information into pieces of information
PrD: Transforming data into data
PrT: Transforming Things into Things (or, a state of a thing into its another state) through a piece of information such as a dog acting to the command of a master.
CnS[s1,x]: Self conscious minds. A mind is capable of being conscious of any object whatever including its own acts. This kind of consciousness is called apperception.

*** 1.3 Information-processing through data processing

Information pieces are entities which are involved in the interaction between the physical world and the world of abstract entities.

What all information pieces have in common is their relation to some physical objects termed as data. The relation involved may be called expressing, representing, articulation,g, formulating, signifying, recording etc.

Verb Recording is considered standard here.

Thus an uttered (spoken or written) sentence is a physical event being a datum to record a proposition. A score, as a sheet of paper covered with notes is a datum to record a piece of music. A drawing is a datum to record the design, say of a house. A configuration of polarized spots on a magnetic medium forms data to record, e.g., a program for computer.

To some extent, the elusiveness of the notion of IP can be remedied by a certain combination of the idea of recording relation with the concept of equivalence (or abstraction class) as defined in logic and set theory.

An equivalence relation is reflexive, symmetric, and transitive. Whenever one has an equivalence relation on a set S, the set can be partitioned into a number of disjoint sets called equivalence classes such that all the members of any one equivalent class bear the relation R to each other but not to any members of S outside that equivalence class.

Cardinal numbers are defined by reference to equivalence classes of equinumerous sets. E.g., number 2 is presented as the set of all pairs. This does not mean that the terms 2 and the class of all pairs denote the same entity. It is certainly true that # > 2 but this does not imply that triples is greater than the class of pairs, hence the respective arguments of the predicate ‘is greater than’ do not prove interchangeable.

The method of introducing abstract objects through equivalence classes, as exemplified above, especially to examine relations between information-processing and data-processing. The idea to be developed is to the effect that pieces of information are abstract entities, in a way assigned to respective equivalence classes of pieces of data.

Author is trying to establish a correspondence between two chains, that of data and that of entities represented by data. The most basic is the correspondence between numeral sequences as data and numbers as entities represented by such sequences. They belong to 2 different domains.
Successor operation turns a number 10 to 11 and in the codomain it turns the numerals ‘10’ into ‘11’. I think it might have been better if the author used the word encode to represent the act of transforming the semantics into the syntax.

The item represented is called an information item and the item used to encode this is called the data.

Data items are recorded in objects such as machines and organisms, and owing to these records information can be stored in objects and processed by them.

Information as the abstract entity and data as the physical entity.

Processing applied to information and data are not independent; they are so related that data-processing is a means of information-processing.

The term ‘sentence’ denote a physical object made from ink, or air waves, or electric impulses, etc. (while the terms ‘proposition’, or ‘statement’, or ‘judgment’ will never appear in this role).

Thus sentences belong to the category of data.

Information pieces recorded in sentences are called propositions, so sentences like 2 + 3 encoded in Roman numerals, Decimal, or Binary representation record the same proposition. Obviously, P is not identical with any of the members of the equivalence class E. Neither with E itself. Were it identical with E, then it would be sensible and true to say, e.g., that P contains the empty class which would be a kind of nonsense.

Then there are information pieces which are abstract entities, each of them being associated with exactly one equivalence class of data.

The notion of blind thinking (caeca cogitatio) was used by Leibniz to indicate the mechanical deduction carried out by computers without a notion of the semantics(?) of say a collection of six objects dividing into two triples.

To sum up the domain of information-processing involves numbers and the domain of data-processing involves numerals. In reasoning, the former consists of propositions, the latter of sentences.

Operation involved in data-processing are formal that is concerned with the form, or structure of strings of characters, that is data, and not with a content. About a reasoning which proceeds solely according to formal rules we say that it is formalized. Should such a reasoning be carried out by a machine, we call it mechanized.

However, there are important difference between these two domains of information-processing which are crucial for our discussion. Data-processing in the sphere of computing, i.e. mechanical calculation on numerals as representing numbers is a usual and unavoidable procedure. But data-processing in the sphere of reasoning, i.e., formalized inference, is a relatively new invention which has a clearly artificial character. Though it has proved necessary for metamathematical research, as well as useful and inspiring for philosophy of mind, it does not prove necessary for efficient reasoning.

In a formalized logical system, we have:

a and b  y infer not y → not a or not base

This is natural language is:

Matter and its motion results in time and space

If matter nd its motion disappeared, there would no longer be any space or time.

** 1.4 Intelligence and model based reasoning

Are there entirely wordless reasonings, such that they do not resort to any piece of a text? If there are, what, then, are the data to be processed? What about the principle that every reasoning is a truth-preserving information-processing, where abstract pieces of information are represented by data as physical entities?

Suppose we say that there are indeed wordless reasonings, what shall we call them? That is the class of reasonings in which information-processing is not supported by any text-processing. Let it be called the class of model-based reasonings. Another designation might be given after Popper in Unended Quest (1982) who observe that we do think in words as well as by means of schemata.

The answer in the affirmative is supported by various kinds of evidence. Due to some natural laws governing organisms, people and animals are capable of forming, e.g., internal pictures of things. Such pictures and similar devices, some of them of more abstract character, can be called models. This term enjoys sufficient generality and, like ‘picture’ involves the notion of similarity in its content.

Why should a craftsman, or an engineer try to name all the states of the engine under repair, when she seems them with her eyes, and can test her unspoken estimations with movements of her fingers? Her reasoning consists in transforming such visual and tactile data in her imagination; seh is not bound to record them in her notebook and adopt predicate calculus for their inferential processing.
In a model based reasoning, the data processed are models, and those are due to records made in a code inside a processing system (e.g., a visual percept encoded in a brain).

These records are also data to represent pieces of information, while in a text-based reasoning information pieces are represented by data formed as sentences. In both cases appear pieces of information of which either truth or falsity can be predicated (e.g, the truth of a percept involved in a model), hence the difference in their representing by verbal data in one case and model data in the other does not affect the nature of reasoning as a truth-preserving transformation.

The point to be vindicated is to the effect that there are reasoning which the reasoner is not aware of, i.e., those which do not occur at the level of conscious reflexion. We shall briefly speak of them that they are not apperceived, taking the term in Leibnizian sense.

Leibniz needed the word perception to denote all acts of living individuals (i.e., substances) as reacting to certain impulses, while fully conscious perceptions were by him distinguished with the specially coined term apperception. He defined it as the reflective knowledge of an inner state, which is not given to all souls, nor at all times to the same soul.

Now the point can be stated briefly that there are unapperceived reasonings in humans and still more briefly there are covert ones. This statement is of great import for the study of intelligence. Provided it is right, the attempts to create artificial intelligence, which would be as close as possible to natural thinking, should not lead toward the text-based mechanical reasonings. Instead, artificial minds should be able to simulate model-based and covert reasonings as proving most efficient in those situations in which the subject of reasoning is not liable to be described in words, and in addition, it has to be grasped in a fraction of the second.

Model-based reasoning is unavoidable in that kind of mental activities which is termed knowing how (in contradistinction to knowing that) by Ryle in The Concept of Mind (1949), and more commonly is called ‘know-how’. This problem was tackled by Herbert Breger in Das Postulat der Explizierbarkeit in der Debatte um die kiinstliche Intelligenz (1988)

There is, actually, no mystery either in model-based or in covert reasonings, they are simply facts of every-day life. Nevertheless, there are philosophical schools such that one of them denies possibility of covert mental acts, and the other possibility of model-based reasonings. From the latter point of view, that of behaviorists, there is a mystery in the conception that some reasonings might be non-verbalized since any thinking is by them construed as an inner silent speaking.

On the other hand, a covert reasoning is regarded as impossibly by the Cartesian philosophy of mind in which the mind is identified with the subject of conscious acts.

According to Descartes, there can be no covert reasonings, as reasoning is the affair of consciousness alone: I reason then and only then, if I know that I reason. There is no necessity for reasonings - meant Descartes - to be recorded in words (hence a model-based reasoning might be admitted), but it is necessary for them to be self-conscious. This is why Descartes denied animals any capability of reasoning; he regarded them as mere automata so unable to make inferences as is unable, say, a clock.

Refer Dipert (1994) - Leibniz’s Discussion of Obscure, Confused, and Inadequate Ideas.

In the Leibnizian perspective reasoning is conceived as a kind of information-processing accomplished through data-processing, while the data are not necessarily linguistic; they may be some records in an organic machine which function as models of those pieces of reality which form the subject-matter of our reasonings.

The main mechanical-intelligence problem can be stated in the terms of processing Encoded Potential Concepts. That there do exist Encoded Potential Concepts in human bodies, is a philosophical hypothesis to motivate a research project.

An instance of EPCs on which attention of scholarly circles was focussed in recent decades is concerned with the notion of linguistic competence as introduced by Noam Chomsky; to start acquiring a language, a human being must have some innate potential notions of language, communication, predication, etc. Even if one does not endorse a materialistic point, it is advantageous to imagine those notions as encoded in our bodies, as a kind of data to be processed, presumably in the central nervous system.

An example of Socrates eliciting the idea of Pythagoras theorem in the boy’s mind is described as an example of potential concept encoded.

Besides grammatical and mathematical encoded potential concepts, there are logico-ontological EPCs which prove indispensable at the start of any language acquisition. Among them are those of a class, individual and of equivalence relation.

These 3 ideas are involved in any act of realizing that an individual object shown in the moment should represent a class to be named so and so, namely the class of those individuals which are identical - in a certain respect with that being produced (i.e., an equivalence class). No communicative act involving ostention would be possible without functioning these logical EPCs, hence they must be innate in every human individual.

A mechanized reasoning with the utmost clearness reveals that any reasoning is a truth-preserving information-processing carried out by means of data-processing, the data being entities as physical in their nature as are electric impulses, or magnetized spots, while the property of truth-preservation is revealed in the explicit application of deductive rules.

The author seems to suggest a program for achieving artificial general intelligence.

One should i) discover mechanisms of model-based reasoning in order to imitate them with artificial devices
ii) furnish such devices with a set of encoded potential concepts similar to that enjoyed by humans
iii) master the process of transforming unconscious, only potential ideas into fully apperceived concepts

* Chapter 2
The Formalization of Arguments in the Middle ages

** 2.1 The contention of the present chapter

The key notions in the story are those of data-processing and information-processing. The former can be entrusted to machines provided that information items (abstract objects) are represented by data (physical objects), hence the results of data processing due ot a machine can be read off by a human as results of information-processing. In a process of reasoning, the formalization consists in recording propositions (information items) as data, and in putting forth inference rules as operating on physical objects.

The notion of Llull as the forerunner of mechanization though no traces of formalization appear in his doctrine as propagated in works on history of logic and history of computer science:

Formale Logik — I. M. Bocheński (1956)
Logic, Machines, and Diagrams — Gardner (1958)

Llull’s candidacy to the status of the principal predecessor of Leibniz and the initiator of the mechanization of arguments proves untenable.

It is true that Leibniz had predecessors in the Middle Ages, but these are rather to be sought in the trend that was in opposition of the Neo-Platonic orientation represented by Llull, namely in the nominalistically-oriented logica modernorum in the late Middle Ages, also called terministic logic because of its semiotic inquiries into the so-called proprietates terminorum.

Risse in his Die Logik der Neuzeit (1964) says:

“The Lull school is to be understood solely through Lull, not by reference to Leibniz. True, Leibniz owes many particular ideas to it but he organizes them in his own way. For with Lullists logic was neither primarily nor essentially connected with mathematics […] (Leibniz’s) calculus ratiocinator rooted in Vieta’s algebra speciosa, is a product of the 17th century, and ars magna played no noticeable role in it”

The opinion that Renaissance mathematicians invented variables must be taken with a grain of salt. It is not groundless to ascribe that idea to Aristotle ast he author of syllogistic schemata. Others, for instance A. N. Whitehead, ascribe the principal merit to Archimedes. When it comes to such a sophisticated concept one may assume in advance that it was developing for centuries stage by stage, and whichever stage is taken to be the turning point in that process, the decision will always be arbitrary.

Dissertation de arte combinatoria in qua, Arithmeticae fundamentis, Complicationum ac Transpositionum Doctrina novis praeceptis exstruitur, et usus ambarum per universum scientarum orbem ostenditur; nova etiam Artis Meditandi seu Logicae Inventionis semina sparguntur. Prefixae est Synopsis totius Traclatus, et additamenti loco Demonstratio existentiae Dei ad Mathematicam certitudinem exacta. - Leibniz (1646)

*** 2.2 Heuristic algorithms in Middle ages

The problem of a certain formalization of arguments was for centuries discussed in terms of the logic of discovery, and hence in terms of processes which we now treat as typically creative and not subject to mechanical procedures.

That was due to a combinatory interpretation of the process of discovery, which involved finitism and formalism. Finitism because effectiveness requires that the number of combinations be finite. Formalism because combinations must be carried out on some discrete object that can be unambiguously identified; these conditions are in a model way satisfied by material symbols owing to their visible shapes (or forms, hence the term ‘formalism’) and discrete arrangement. That finitistic formalism started in the late Middle Ages and culminated in the 17th century.

The position of theory of reasoning in the structure of traditional logic. Tripartite ordering schema ordered by singling out three hierarchically arranged operations of the mind:

Fundamental operation: Grasping things by concepts. Simplex apprehensio. It is the simplest one in the sense that in that part of the act of grasping thing which occurs at the level of consciousness, and so is accessible to introspection, we do not perceive any components that could be clearly isolated.

The second operation of the mind, second in the sense that it assumes the existence of concepts and is more complex than conceptualization, is the formation of judgement. According to Lullistic logic, nad scholastic logic in general, it consists in the combination of concepts into a judgement (judicium). This had suggested to Llullus the idea of a combinatorial procedure of generating judgements.

This is said to be an Utopian plan because in the construction of sentences we can admit predicates with an arbitrary number of arguments; and the imposition of any constraint upon the number of arguments would either be conventional or appeal to intuitions that are far from being mechanizable.

A judgement was treated as an invariable tripartite structure consisting of the subject, the copula, and the predicate, the copula expressing either affirmation ('is') or negation ('is not'). This is how a judgment (protasis) was understood by Aristotle. For a properly limited dictionary of terms (i.e., expressions which can function as either the subject or the predicate) this yields a realistic algorithm to produce the list of all possible judgements. From such a list one would then have to choose those judgements which are true and as such are suitable as premises of those syllogisms that are to yield truth. Today we find it difficult to imagine what algorithm which is not a proof (because the proof, e.g., a syllogism, comes later as the operation that is next to the formation of judgments) could be constructed for that purpose: empirical truths are beyond the reach of any algorithms whatever. The point is, however, that for the representatives of the Platonic-Aristotelian views (which came to be opposed by modern empiricism) truth in the strict sene of the term was identical with necessary truth or (approximately) analytic truth, that is, the glorified logic of discovery, ars inventionis, which preoccupied many outstanding minds in the Middle Ages and the Renaissance until 18th century.

The third operation of the mind, the most complex one in the sense that it assumes the two preceding ones and brings the most complex product, consists in the proof, construed in the Aristotelian logic as a syllogism. This identification of proofs with syllogism in that tradition is essential for the present discussion because syllogistic combinatorics, which is the foundation of mechanization can then be identified with the theory of proof taken as a whole, which would allow one to conclude that all proofs can be mechanized.

By a proof Aristotle means a syllogism that creates scientific knowledge. Hence if knowledge is such as we have stated, then the premises of demonstrative knowledge must be true, primitive, direct, better known (than the conclusion) and must be its cause.

According to Aristotle’s interpretation the difference between a proof and a syllogism is epistemological and not formal logical in character: a proof is a syllogism hose premises meet the epistemological conditions of scientific knowledge (episteme) as distinguished from common belief (doxa). Hence in science there are no proofs other than syllogisms, even though not every syllogism is a proof. Thus syllogistic exhausts the whole formal logical part of the theory of scientific proofs to which, in Aristotle’s intention, his Prior Analytics (concerned with formal logic) and Posterior Analytics (concerned with methodology of science) were dedicated.

Someone who knows the history of Greek and later mathematicians might object the point that logicians of that time identified a proof with a syllogism. They must have known the procedure employed in proofs by Euclid and other mathematicians which hardly resembled syllogistic forms. If so, why did they claim that every proof should be a syllogistic inference?

Nowadays it is well-known fact that syllogistic can be interpreted in the monadic predicate calculus which is a decidable theory. This fact may have been intuitively sensed in practicing syllogistic inferences, and together with identifying the whole of logic with syllogistic that might have led to the belief in the possibility of mechanizing all reasonings. This belief seems to have been favoured by other factors in the cultural context in which logic existed for two millennia. That context involved two philosophical tendencies, namely finitism and formalism.

In the Greek philosophy of mathematics finitism established its place for good owing to the paradoxes of Zeno of Elea (490-430 B.C.) which showed how formidably perplex are the problems resulting from the concept of (actual) infinity. That was reinforced by the authority of Aristotle who in his Physics and Metaphysics advanced arguments, to be later repeated for centuries, against the existence of actually infinite domains. The opinion was also represented in antiquity by other schools (TODO: Research which ones held to this idea) but Aristotle’s voice sounded more loudly, especially when supported by Christian thought: the major part of his representatives reserve infinity for the Creator alone while denying it to the creature. The finitist camp included such influential authors in Christian antiquity as Origen (185 – 254), with whom centuries later Cantor himself would engage in vehement disputes, Proclus of Constantinopole (410 – 485), an influential commentator of Euclid, and - in the period of the flourishing of medieval philosophy - Thomas Aquinas (1225 – 1247), the greatest Christian Aristotelian.

That camp did not include Augustine of Hippo (354 – 430), but his standpoint, voiced only in connection with other problems and hence likely to be overlooked, was fully understood probably only by Cantor (who sought in him as ally in polemics with contemporary theologians).

Thus the intellectual climate in which the Lullists were active favoured finitism. The view which implied the finiteness of both the domain of individuals and the set of concepts provided people with reasons to postulate the decidability of the system of human knowledge. True statements would be deducible from a finite set of first principles (as Aristotle claimed), and false ones would be refutable by demonstrating that they contradict those principles.

The ideas of the potential infinity of human cognition, of the limits of verbalization, of the approximative nature of scientific theories, and the like, have become familiar to the modern mind only recently.

As long as it was believed that concepts and judgements had adequate mappings in language, on the one-to-one basis, there were reasons to believe that thoughts could be fully replaced by words, and these, being material objects, could be processed mechanically.

Augustinism opposed Aristotelianism both by its infinitism and its doctrine of illumination, which stressed the intuitive, non-mechanizable elements of cognition; that tendency even more manifested itself in the gnostic movements. But as for the problems with which we are concerned here the essential point is that both the finitistic and the formalistic trend were firmly rooted in the medieval thought.

Inventio medii: The problem of finding the middle term.

It was typical of logica inventionis, that is, the logic of discovery, postulated also by Lull, who wrote, among other things, the treatise entitled Ars inventiva veritatis.

Averroes (1126 – 1198) one of the greatest Arab Aristotelians, was the first medieval author known to have coped with the issue. On the other hand, Albert the Great (1193–1280) was the first who, following Averroes, assimilated that problem to Christian scholasticism. Albert’s another merit consisted in that he took up, after the Arabs, combinatorial investigations concerning the number of all possible syllogistic structures in order to find all possible correct syllogisms. A similar combinatorial approach can be found in hte Jewish logician named Albalag, who was a contemporary of Lullus and like the latter was active in northern Spain. As can be seen, when it comes to the combinatorial approach, Lull was by far not the first among the schoolmen, and if he did not take that problem for instance from Albert the Great, then he must have most probably owed it, as Albert did, to the Arabs.

Dzieje filozofii europejskiej w XV wieku. Tom II: Wiedza - S. Swiezawski (1974)

Among the authors who took up those problems after Albert the Great special mention is usually given to George of Brussels and Thomas Bricot. They belonged tot he nominalistic trend in the 15th century (logica modernorum), which had originated with he great Oxford masters, William of Ockham and Richard Suiseth, whose teachigns transferred tot he Continent by Paul of Venice took strong roots at the universities in northern Italy and central and eastern Europe, including Prague, Cracow, and Leipzig.