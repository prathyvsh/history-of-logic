* Mechanization of Reasoning in a Historical Perspective

Started reading:
21 August 21 8:19 pm IST

The idea of formalization of reasoning starts from the middle ages and reached its maturity in modern logic that is in Frege → Peano → Russell and Whitehead → Hilbert → Lukasiewicz → Gödel → Tarski → Jaśkowski → Leśniewski etc.

Elimination of quantifiers results in reducing whole of logic into binary algebra.

Formalization of logic effectively resulted in us pursuing inquiry into the nature of intelligence by providing us a suitable base.

Reasoning stands for deduction in this volume.

A thinking machine is an information-processing system that has a hardware component, the processing being aimed at growth of information; Information processing is found in a continuous interplay with data processing, the former being construed as operating on abstract objects (numbers, propositions, etc.), the latter as operating on physical tokes (e.g., numerals produced with ink, or by magnetization of some spots, etc.) which represent pieces of information.

Logic is the theory which deals with a special kind of information processing, namely that which preserves the truth of information.

Two sense of mechanization:

1/ Mechanization as synonymous with formalization. Formalization as in Post production systems, where you can deduce theorems through syntactic manipulations
2/ Mechanization in the narrow sense that there exists a device and a software to operate on it in order to process sentences (syntactically defined string of tokens) according to an algorithm involved in the given formalization.

Formalization or mechanization in the sense of 2/ consists in using a device and a mapping to this device of an abstract machine.

The work of Shannon in interpreting circuits as performing boolean logic allowed for the method of reducing predicate logic to propositional logic due to Skolem, Hilbert, and others which enabled the use of binary digital circuitry for mechanizing proofs in predicate logic.

One more significant step in this direction is the cut-free formalisms of Herbrand and Gentzen of the predicate-calculus. The cut is a schema representing a number of inference rules, one of them being the familiar modus ponendo ponens, whose use in a proof requires some ingenuity on the side of the reasoner to identify the premises from which the conclusion is being cut must be found.

On the other hand, in a proof produced with the cut-free formalism, each step is determined by the syntactic structure of the formula processed, hence the need for intervention is extremely reduced, and the whole procedure becomes fairly mechanical.

Gentzen demonstrated that any proof involving the cut rule can be transformed into a proof in which the cut does not appear, and this implies availability of a mechanical procedure for any proof formalized in predicate logic.

CU = <Information + Data + Machines?;
       Natural Machine, Artificial Machine, Selves;
       Rec[d,i]
       PrI[i,j]
       PrD[d,e]
       PrT[t1,t2,i],
       Cns[s1,x]>


Rec[d,i]: Piece of data d is assigned the information i.
PrI[i,j]: Transforming information into pieces of information
PrD: Transforming data into data
PrT: Transforming Things into Things (or, a state of a thing into its another state) through a piece of information such as a dog acting to the command of a master.
CnS[s1,x]: Self conscious minds. A mind is capable of being conscious of any object whatever including its own acts. This kind of consciousness is called apperception.

*** 1.3 Information-processing through data processing

Information pieces are entities which are involved in the interaction between the physical world and the world of abstract entities.

What all information pieces have in common is their relation to some physical objects termed as data. The relation involved may be called expressing, representing, articulation,g, formulating, signifying, recording etc.

Verb Recording is considered standard here.

Thus an uttered (spoken or written) sentence is a physical event being a datum to record a proposition. A score, as a sheet of paper covered with notes is a datum to record a piece of music. A drawing is a datum to record the design, say of a house. A configuration of polarized spots on a magnetic medium forms data to record, e.g., a program for computer.

To some extent, the elusiveness of the notion of IP can be remedied by a certain combination of the idea of recording relation with the concept of equivalence (or abstraction class) as defined in logic and set theory.

An equivalence relation is reflexive, symmetric, and transitive. Whenever one has an equivalence relation on a set S, the set can be partitioned into a number of disjoint sets called equivalence classes such that all the members of any one equivalent class bear the relation R to each other but not to any members of S outside that equivalence class.

Cardinal numbers are defined by reference to equivalence classes of equinumerous sets. E.g., number 2 is presented as the set of all pairs. This does not mean that the terms 2 and the class of all pairs denote the same entity. It is certainly true that # > 2 but this does not imply that triples is greater than the class of pairs, hence the respective arguments of the predicate ‘is greater than’ do not prove interchangeable.

The method of introducing abstract objects through equivalence classes, as exemplified above, especially to examine relations between information-processing and data-processing. The idea to be developed is to the effect that pieces of information are abstract entities, in a way assigned to respective equivalence classes of pieces of data.

Author is trying to establish a correspondence between two chains, that of data and that of entities represented by data. The most basic is the correspondence between numeral sequences as data and numbers as entities represented by such sequences. They belong to 2 different domains.
Successor operation turns a number 10 to 11 and in the codomain it turns the numerals ‘10’ into ‘11’. I think it might have been better if the author used the word encode to represent the act of transforming the semantics into the syntax.

The item represented is called an information item and the item used to encode this is called the data.

Data items are recorded in objects such as machines and organisms, and owing to these records information can be stored in objects and processed by them.

Information as the abstract entity and data as the physical entity.

Processing applied to information and data are not independent; they are so related that data-processing is a means of information-processing.

The term ‘sentence’ denote a physical object made from ink, or air waves, or electric impulses, etc. (while the terms ‘proposition’, or ‘statement’, or ‘judgment’ will never appear in this role).

Thus sentences belong to the category of data.

Information pieces recorded in sentences are called propositions, so sentences like 2 + 3 encoded in Roman numerals, Decimal, or Binary representation record the same proposition. Obviously, P is not identical with any of the members of the equivalence class E. Neither with E itself. Were it identical with E, then it would be sensible and true to say, e.g., that P contains the empty class which would be a kind of nonsense.

Then there are information pieces which are abstract entities, each of them being associated with exactly one equivalence class of data.

The notion of blind thinking (caeca cogitatio) was used by Leibniz to indicate the mechanical deduction carried out by computers without a notion of the semantics(?) of say a collection of six objects dividing into two triples.

To sum up the domain of information-processing involves numbers and the domain of data-processing involves numerals. In reasoning, the former consists of propositions, the latter of sentences.

Operation involved in data-processing are formal that is concerned with the form, or structure of strings of characters, that is data, and not with a content. About a reasoning which proceeds solely according to formal rules we say that it is formalized. Should such a reasoning be carried out by a machine, we call it mechanized.

However, there are important difference between these two domains of information-processing which are crucial for our discussion. Data-processing in the sphere of computing, i.e. mechanical calculation on numerals as representing numbers is a usual and unavoidable procedure. But data-processing in the sphere of reasoning, i.e., formalized inference, is a relatively new invention which has a clearly artificial character. Though it has proved necessary for metamathematical research, as well as useful and inspiring for philosophy of mind, it does not prove necessary for efficient reasoning.

In a formalized logical system, we have:

a and b  y infer not y → not a or not base

This is natural language is:

Matter and its motion results in time and space

If matter nd its motion disappeared, there would no longer be any space or time.

** 1.4 Intelligence and model based reasoning

Are there entirely wordless reasonings, such that they do not resort to any piece of a text? If there are, what, then, are the data to be processed? What about the principle that every reasoning is a truth-preserving information-processing, where abstract pieces of information are represented by data as physical entities?

Suppose we say that there are indeed wordless reasonings, what shall we call them? That is the class of reasonings in which information-processing is not supported by any text-processing. Let it be called the class of model-based reasonings. Another designation might be given after Popper in Unended Quest (1982) who observe that we do think in words as well as by means of schemata.

The answer in the affirmative is supported by various kinds of evidence. Due to some natural laws governing organisms, people and animals are capable of forming, e.g., internal pictures of things. Such pictures and similar devices, some of them of more abstract character, can be called models. This term enjoys sufficient generality and, like ‘picture’ involves the notion of similarity in its content.

Why should a craftsman, or an engineer try to name all the states of the engine under repair, when she seems them with her eyes, and can test her unspoken estimations with movements of her fingers? Her reasoning consists in transforming such visual and tactile data in her imagination; seh is not bound to record them in her notebook and adopt predicate calculus for their inferential processing.
In a model based reasoning, the data processed are models, and those are due to records made in a code inside a processing system (e.g., a visual percept encoded in a brain).

These records are also data to represent pieces of information, while in a text-based reasoning information pieces are represented by data formed as sentences. In both cases appear pieces of information of which either truth or falsity can be predicated (e.g, the truth of a percept involved in a model), hence the difference in their representing by verbal data in one case and model data in the other does not affect the nature of reasoning as a truth-preserving transformation.

The point to be vindicated is to the effect that there are reasoning which the reasoner is not aware of, i.e., those which do not occur at the level of conscious reflexion. We shall briefly speak of them that they are not apperceived, taking the term in Leibnizian sense.

Leibniz needed the word perception to denote all acts of living individuals (i.e., substances) as reacting to certain impulses, while fully conscious perceptions were by him distinguished with the specially coined term apperception. He defined it as the reflective knowledge of an inner state, which is not given to all souls, nor at all times to the same soul.

Now the point can be stated briefly that there are unapperceived reasonings in humans and still more briefly there are covert ones. This statement is of great import for the study of intelligence. Provided it is right, the attempts to create artificial intelligence, which would be as close as possible to natural thinking, should not lead toward the text-based mechanical reasonings. Instead, artificial minds should be able to simulate model-based and covert reasonings as proving most efficient in those situations in which the subject of reasoning is not liable to be described in words, and in addition, it has to be grasped in a fraction of the second.

Model-based reasoning is unavoidable in that kind of mental activities which is termed knowing how (in contradistinction to knowing that) by Ryle in The Concept of Mind (1949), and more commonly is called ‘know-how’. This problem was tackled by Herbert Breger in Das Postulat der Explizierbarkeit in der Debatte um die kiinstliche Intelligenz (1988)

There is, actually, no mystery either in model-based or in covert reasonings, they are simply facts of every-day life. Nevertheless, there are philosophical schools such that one of them denies possibility of covert mental acts, and the other possibility of model-based reasonings. From the latter point of view, that of behaviorists, there is a mystery in the conception that some reasonings might be non-verbalized since any thinking is by them construed as an inner silent speaking.

On the other hand, a covert reasoning is regarded as impossibly by the Cartesian philosophy of mind in which the mind is identified with the subject of conscious acts.

According to Descartes, there can be no covert reasonings, as reasoning is the affair of consciousness alone: I reason then and only then, if I know that I reason. There is no necessity for reasonings - meant Descartes - to be recorded in words (hence a model-based reasoning might be admitted), but it is necessary for them to be self-conscious. This is why Descartes denied animals any capability of reasoning; he regarded them as mere automata so unable to make inferences as is unable, say, a clock.

Refer Dipert (1994) - Leibniz’s Discussion of Obscure, Confused, and Inadequate Ideas.

In the Leibnizian perspective reasoning is conceived as a kind of information-processing accomplished through data-processing, while the data are not necessarily linguistic; they may be some records in an organic machine which function as models of those pieces of reality which form the subject-matter of our reasonings.

The main mechanical-intelligence problem can be stated in the terms of processing Encoded Potential Concepts. That there do exist Encoded Potential Concepts in human bodies, is a philosophical hypothesis to motivate a research project.

An instance of EPCs on which attention of scholarly circles was focussed in recent decades is concerned with the notion of linguistic competence as introduced by Noam Chomsky; to start acquiring a language, a human being must have some innate potential notions of language, communication, predication, etc. Even if one does not endorse a materialistic point, it is advantageous to imagine those notions as encoded in our bodies, as a kind of data to be processed, presumably in the central nervous system.

An example of Socrates eliciting the idea of Pythagoras theorem in the boy’s mind is described as an example of potential concept encoded.

Besides grammatical and mathematical encoded potential concepts, there are logico-ontological EPCs which prove indispensable at the start of any language acquisition. Among them are those of a class, individual and of equivalence relation.

These 3 ideas are involved in any act of realizing that an individual object shown in the moment should represent a class to be named so and so, namely the class of those individuals which are identical - in a certain respect with that being produced (i.e., an equivalence class). No communicative act involving ostention would be possible without functioning these logical EPCs, hence they must be innate in every human individual.

A mechanized reasoning with the utmost clearness reveals that any reasoning is a truth-preserving information-processing carried out by means of data-processing, the data being entities as physical in their nature as are electric impulses, or magnetized spots, while the property of truth-preservation is revealed in the explicit application of deductive rules.

The author seems to suggest a program for achieving artificial general intelligence.

One should i) discover mechanisms of model-based reasoning in order to imitate them with artificial devices
ii) furnish such devices with a set of encoded potential concepts similar to that enjoyed by humans
iii) master the process of transforming unconscious, only potential ideas into fully apperceived concepts

* Chapter 2
The Formalization of Arguments in the Middle ages

** 2.1 The contention of the present chapter

The key notions in the story are those of data-processing and information-processing. The former can be entrusted to machines provided that information items (abstract objects) are represented by data (physical objects), hence the results of data processing due ot a machine can be read off by a human as results of information-processing. In a process of reasoning, the formalization consists in recording propositions (information items) as data, and in putting forth inference rules as operating on physical objects.

The notion of Llull as the forerunner of mechanization though no traces of formalization appear in his doctrine as propagated in works on history of logic and history of computer science:

Formale Logik — I. M. Bocheński (1956)
Logic, Machines, and Diagrams — Gardner (1958)

Llull’s candidacy to the status of the principal predecessor of Leibniz and the initiator of the mechanization of arguments proves untenable.

It is true that Leibniz had predecessors in the Middle Ages, but these are rather to be sought in the trend that was in opposition of the Neo-Platonic orientation represented by Llull, namely in the nominalistically-oriented logica modernorum in the late Middle Ages, also called terministic logic because of its semiotic inquiries into the so-called proprietates terminorum.

Risse in his Die Logik der Neuzeit (1964) says:

“The Lull school is to be understood solely through Lull, not by reference to Leibniz. True, Leibniz owes many particular ideas to it but he organizes them in his own way. For with Lullists logic was neither primarily nor essentially connected with mathematics […] (Leibniz’s) calculus ratiocinator rooted in Vieta’s algebra speciosa, is a product of the 17th century, and ars magna played no noticeable role in it”

The opinion that Renaissance mathematicians invented variables must be taken with a grain of salt. It is not groundless to ascribe that idea to Aristotle ast he author of syllogistic schemata. Others, for instance A. N. Whitehead, ascribe the principal merit to Archimedes. When it comes to such a sophisticated concept one may assume in advance that it was developing for centuries stage by stage, and whichever stage is taken to be the turning point in that process, the decision will always be arbitrary.

Dissertation de arte combinatoria in qua, Arithmeticae fundamentis, Complicationum ac Transpositionum Doctrina novis praeceptis exstruitur, et usus ambarum per universum scientarum orbem ostenditur; nova etiam Artis Meditandi seu Logicae Inventionis semina sparguntur. Prefixae est Synopsis totius Traclatus, et additamenti loco Demonstratio existentiae Dei ad Mathematicam certitudinem exacta. - Leibniz (1646)

*** 2.2 Heuristic algorithms in Middle ages

The problem of a certain formalization of arguments was for centuries discussed in terms of the logic of discovery, and hence in terms of processes which we now treat as typically creative and not subject to mechanical procedures.

That was due to a combinatory interpretation of the process of discovery, which involved finitism and formalism. Finitism because effectiveness requires that the number of combinations be finite. Formalism because combinations must be carried out on some discrete object that can be unambiguously identified; these conditions are in a model way satisfied by material symbols owing to their visible shapes (or forms, hence the term ‘formalism’) and discrete arrangement. That finitistic formalism started in the late Middle Ages and culminated in the 17th century.

The position of theory of reasoning in the structure of traditional logic. Tripartite ordering schema ordered by singling out three hierarchically arranged operations of the mind:

Fundamental operation: Grasping things by concepts. Simplex apprehensio. It is the simplest one in the sense that in that part of the act of grasping thing which occurs at the level of consciousness, and so is accessible to introspection, we do not perceive any components that could be clearly isolated.

The second operation of the mind, second in the sense that it assumes the existence of concepts and is more complex than conceptualization, is the formation of judgement. According to Lullistic logic, nad scholastic logic in general, it consists in the combination of concepts into a judgement (judicium). This had suggested to Llullus the idea of a combinatorial procedure of generating judgements.

This is said to be an Utopian plan because in the construction of sentences we can admit predicates with an arbitrary number of arguments; and the imposition of any constraint upon the number of arguments would either be conventional or appeal to intuitions that are far from being mechanizable.

A judgement was treated as an invariable tripartite structure consisting of the subject, the copula, and the predicate, the copula expressing either affirmation ('is') or negation ('is not'). This is how a judgment (protasis) was understood by Aristotle. For a properly limited dictionary of terms (i.e., expressions which can function as either the subject or the predicate) this yields a realistic algorithm to produce the list of all possible judgements. From such a list one would then have to choose those judgements which are true and as such are suitable as premises of those syllogisms that are to yield truth. Today we find it difficult to imagine what algorithm which is not a proof (because the proof, e.g., a syllogism, comes later as the operation that is next to the formation of judgments) could be constructed for that purpose: empirical truths are beyond the reach of any algorithms whatever. The point is, however, that for the representatives of the Platonic-Aristotelian views (which came to be opposed by modern empiricism) truth in the strict sene of the term was identical with necessary truth or (approximately) analytic truth, that is, the glorified logic of discovery, ars inventionis, which preoccupied many outstanding minds in the Middle Ages and the Renaissance until 18th century.

The third operation of the mind, the most complex one in the sense that it assumes the two preceding ones and brings the most complex product, consists in the proof, construed in the Aristotelian logic as a syllogism. This identification of proofs with syllogism in that tradition is essential for the present discussion because syllogistic combinatorics, which is the foundation of mechanization can then be identified with the theory of proof taken as a whole, which would allow one to conclude that all proofs can be mechanized.

By a proof Aristotle means a syllogism that creates scientific knowledge. Hence if knowledge is such as we have stated, then the premises of demonstrative knowledge must be true, primitive, direct, better known (than the conclusion) and must be its cause.

According to Aristotle’s interpretation the difference between a proof and a syllogism is epistemological and not formal logical in character: a proof is a syllogism hose premises meet the epistemological conditions of scientific knowledge (episteme) as distinguished from common belief (doxa). Hence in science there are no proofs other than syllogisms, even though not every syllogism is a proof. Thus syllogistic exhausts the whole formal logical part of the theory of scientific proofs to which, in Aristotle’s intention, his Prior Analytics (concerned with formal logic) and Posterior Analytics (concerned with methodology of science) were dedicated.

Someone who knows the history of Greek and later mathematicians might object the point that logicians of that time identified a proof with a syllogism. They must have known the procedure employed in proofs by Euclid and other mathematicians which hardly resembled syllogistic forms. If so, why did they claim that every proof should be a syllogistic inference?

Nowadays it is well-known fact that syllogistic can be interpreted in the monadic predicate calculus which is a decidable theory. This fact may have been intuitively sensed in practicing syllogistic inferences, and together with identifying the whole of logic with syllogistic that might have led to the belief in the possibility of mechanizing all reasonings. This belief seems to have been favoured by other factors in the cultural context in which logic existed for two millennia. That context involved two philosophical tendencies, namely finitism and formalism.

In the Greek philosophy of mathematics finitism established its place for good owing to the paradoxes of Zeno of Elea (490-430 B.C.) which showed how formidably perplex are the problems resulting from the concept of (actual) infinity. That was reinforced by the authority of Aristotle who in his Physics and Metaphysics advanced arguments, to be later repeated for centuries, against the existence of actually infinite domains. The opinion was also represented in antiquity by other schools (TODO: Research which ones held to this idea) but Aristotle’s voice sounded more loudly, especially when supported by Christian thought: the major part of his representatives reserve infinity for the Creator alone while denying it to the creature. The finitist camp included such influential authors in Christian antiquity as Origen (185 – 254), with whom centuries later Cantor himself would engage in vehement disputes, Proclus of Constantinople (410 – 485), an influential commentator of Euclid, and - in the period of the flourishing of medieval philosophy - Thomas Aquinas (1225 – 1247), the greatest Christian Aristotelian.

That camp did not include Augustine of Hippo (354 – 430), but his standpoint, voiced only in connection with other problems and hence likely to be overlooked, was fully understood probably only by Cantor (who sought in him as ally in polemics with contemporary theologians).

Thus the intellectual climate in which the Lullists were active favoured finitism. The view which implied the finiteness of both the domain of individuals and the set of concepts provided people with reasons to postulate the decidability of the system of human knowledge. True statements would be deducible from a finite set of first principles (as Aristotle claimed), and false ones would be refutable by demonstrating that they contradict those principles.

The ideas of the potential infinity of human cognition, of the limits of verbalization, of the approximative nature of scientific theories, and the like, have become familiar to the modern mind only recently.

As long as it was believed that concepts and judgements had adequate mappings in language, on the one-to-one basis, there were reasons to believe that thoughts could be fully replaced by words, and these, being material objects, could be processed mechanically.

Augustinism opposed Aristotelianism both by its infinitism and its doctrine of illumination, which stressed the intuitive, non-mechanizable elements of cognition; that tendency even more manifested itself in the gnostic movements. But as for the problems with which we are concerned here the essential point is that both the finitistic and the formalistic trend were firmly rooted in the medieval thought.

Inventio medii: The problem of finding the middle term.

It was typical of logica inventionis, that is, the logic of discovery, postulated also by Lull, who wrote, among other things, the treatise entitled Ars inventiva veritatis.

Averroes (1126 – 1198) one of the greatest Arab Aristotelians, was the first medieval author known to have coped with the issue. On the other hand, Albert the Great (1193–1280) was the first who, following Averroes, assimilated that problem to Christian scholasticism. Albert’s another merit consisted in that he took up, after the Arabs, combinatorial investigations concerning the number of all possible syllogistic structures in order to find all possible correct syllogisms. A similar combinatorial approach can be found in hte Jewish logician named Albalag, who was a contemporary of Lullus and like the latter was active in northern Spain. As can be seen, when it comes to the combinatorial approach, Lull was by far not the first among the schoolmen, and if he did not take that problem for instance from Albert the Great, then he must have most probably owed it, as Albert did, to the Arabs.

Dzieje filozofii europejskiej w XV wieku. Tom II: Wiedza - S. Swiezawski (1974)

Among the authors who took up those problems after Albert the Great special mention is usually given to George of Brussels and Thomas Bricot. They belonged tot he nominalistic trend in the 15th century (logica modernorum), which had originated with he great Oxford masters, William of Ockham and Richard Suiseth, whose teachigns transferred tot he Continent by Paul of Venice took strong roots at the universities in northern Italy and central and eastern Europe, including Prague, Cracow, and Leipzig.

Thomas Bricot presents a sort of algorithm on how to find the middle term. The full set of such rules in the form of a graphical schema, nicknamed pons asinorum, was given by Petrus Tartaretus in his commentary to Poryphyry’s Isagoge and to Aristotle’s logical writings; the diagram itself is supposed to have been constructed ca. 1480. The term ‘bridge of asses’, to this day preserved, oscilattes between two meanings. Tartaretus himself, in his desire to prevent his students from experiencing anxiety in view of the complexity of the diagram, explained that such anxiety would be as groundless as that which is felt by asses which are to enter a bridge, because that diagram is to ensure a safe passage, and not to render it difficult. In the other meaning, until today to be found in dictionaries, one takes into consideration the feature of facility which marks algorithms, that is, mechanical procedures that do not require inventiveness on the part of the user, and as such are manageable even by proverbial asses.

The picture that emerges from the foregoing overview does not confirm the opinion about Lull’s role in shaping the idea of the mechanization of arguments. The key role was played by late medieval nominalists with their definitions of logic as pertaining to terms, and hence physical objects on which mechanical operations can be performed (note in this connection that the leading nominalists, such as Ockham, Suiseth, and Buridan, were also forerunners of modern mechanics). It was nothing else than terministic logic, that logica modernorum which formed the main link between Aristotle (whose texts admit a “terministic” interpretation but do not forejudge it) and Leibniz, who, after having found himself, together with his Leipzig masters and Thomas Hobbes in the sphere of nominalistic inspiration, transformed it in his vision of logic embedded in a mechanism.

There was vehement polemics with Aristotelian and scholastic logic which took place in the Renaissance and the 17th century. What strikes us today as the value represented by the formalist wing of logic, namely the approch to possible mechanization, was severely criticized, at first by numerous humanists, who postulated that logic should “come closer to life”, the postulate marked by their practical and psychologistic attitude, typical of rhetoric.

At the same time there was criticism in the vein of Francis Bacon, which demanded that logic should discover hard natural facts ad not mere terms (such as the middle term to be found with medieval recipes).

In the 17th century, in turn, the formalistic trend had its main opponents in Descartes and his followers, who ascribed to logic the role of the healer of minds. According to that programme logic was to protect human minds against deviations, including the scholastic and formalistic ones, and to make them capable of finding the truth. In that respect it also deserved the name of logicae inventionis, but in a new sense, namely that of Cartesian rules for the seacrh for the truth with the natural light of reason, which formed a certain linkage to the Platonic/Augustinian tradition with its key concept of illumination.

One could hardly deny the pertinence of Cartesian criticism when it was aimed att he triviality and sterility of such formal systems of rules as pons asinorum. In fact, an acute human mind never makes use of them in its search for the truth. Today we realize even better than the Cartesians did that inventive intuition is what can never be mechanized. Hence, should the only task of logic consist in the guidance or reinforcement of human minds, one could, and even had to, agree with the Cartesian critique of scholastic formalism. Yet it turned out, and that just in our century, that logic can be used to mechanical processing of knowledge.

Bridge of asses refers to the relations of inclusion and mutual exclusion which is present in the extensions of the term involved in the thesaurus of a language with a grammar and a vocabulary. ALl this is, self-evidently, the same as the formulation of a certain axiom system of language, that system playing the role of axiomatic rules in the sense as understood by Ajdukiewicz in Sprache und Sinn (1934). The axioms in that system should be independent of one another, which makes the system desirably economical. Endowed in this way the computer becomes a master of ars inventionis in the sense of scholastic formalists, for it can faultlessly find the middle term required in the proof of a given thesis. It will do that by searching the vocabulary and accepting for a given proof those terms which bear to one another extensional relations required by the rules of the bridge of asses.

The term ‘follows’ in it being interpreted in the sense of set theoretical inclusion. This makes me think of filters in lattice theory, where an ultrafilter stands as evidence for weaker truths above it.

For our discussion it is of minor importance that the formal apparatus of scholastic logic is so limited as compared with the needs of the automatic processing of knowledge. The essential poitn to be seen in the similarity of programmes, which makes us understand the rules of logic as rules of operations on physical objects of a definite form (hence formalism) which at the point of departure constitue a discrete and finite set (finitism). That approach to logic, which may be read between the lines in Aristotle, was consciously taken by medieval formalists, and then developed by Leibniz and (independently of him) by later authors, especially by George Boole; it was Boole likewise Leibniz from whom Frege took the idea of his logical enterprise.

Frege’s 1880/81 manuscript contains a comparison between Leibniz’s, Boole’s and his own approach.

*** 2.3 The role of Lull and Lullism

Ramon or Raymundus Lullus was born in 1232 in the capital of Majorca, an island recovered from the Arabs in 1229 by the Catalan army led by Jacobus I or Aragonia.

Lull’s famous invention discussed in the history of logic was called ars magna by himself. It covers a certain technique of forming judgements and also something which might be compared to an axiomatic system in the field of philosophy and theology.

Most formal part of Lull’s art, which might be termed combinatorial syllogistic, is to be found earlier in Albert the Great; the latter took it over from still earlier Arab authors, and the problem originated with Aristotle who asked about the way of finding the middle term. The fact that Lull’s thoughts were imbued with that methodology can be sufficiently explained by his being versed in Arab logic, which he presented in his earlier writings, such as Logica del Gazzali (the Catalan version of the name of an Arab logician), written by him in Arabic and translated by himself into Latin and Catalan.

The core of the argumentation of Lull in the most concise form is tob e found in the discussion which he had with an Arab dignitary during one of his later missionary travels (the discussion ended in Lull’s imprisonment at the moment when his Arab opponent ran short of arguments).

Lull had earlier agreed with his opponent that their discussion would take as the starting point the common belief in goodness as one of the principal attributes (dignities) of God. The belief that God has all those attributes was part of the common heritage of both Christian and Arab metaphysics of those times, shaped in the Neo-Platonic melitng pot. By departing from that common point Lull argued in his discussion, like in every other one in which he engaged, that in accordance with the Neo-Platonic principle (also common to his opponents) bonum est diffusivum sui, the goodness of God must spread of necessity. If that diffusion did not consist in giving rise to the second and third person of Trinity, then God would have to manifest himself only by the creation of the world, and then the manifestation of his goodness would depend on something which is not necessary, and hence imperfect and thus unworthy of God. In his first speech in Tunisia Lullus expressed that by saying that without the internal activeness within the Trinity God’s dignitates, including his goodness, would be idle and hence would not be as perfect as it becomes the Absolute.

It is pointed out in the book that the reasonings one derives out of the judgement could be both correct and wrong ones as there is no mechanical procedure of verification of certain formations as deriving only the true statements in Lull’s Ars Magna.

By bringing in a combinatorial exhaustion of the space, Lull intended for his machines to allow to reach those combinations which are fundamental and natural, their truth being grasped with “the natural light of the mind” (to use the Cartesian phrase, which is due to the same Augustinian tradition).

In a sense, the procedure like that may be interpreted as the search for a third term. In a debate, say, one denies that human acts are subjected to deterministic causation, hence are not free, and in order to prove his point one resorts to the concept of divine concurring as one that yields the middle term. Thus one argues: “Every act of human will is supported by divine concurring; every act supported by divine concurring is free, hence every act of human will is free.” Note, however, that this kind of pursuing the third term is quite a different thing than that prescribed, e.g., for the pons asinorum procedure. No rules of the formal correctness of an argument are at stake but merely a heuristic device to activate memory, and so to find a concept which otherwise might have remained unnoticed. Once noticed, the concepts in question are perceived as connected in a necessary way, on the basis of an intellectual intuition. This should put an end to the legend of Lull as an author of a program of arguments mechanization.

In Part XIII, the last one, in which the method of using the Art is discussed, we find an advice for the teacher that he should make it plain to his disciples that the Art cannot be used without recourse to its close companions, that is, the reason, the subtlety of intellect, and good intentions. In particular, the reference to good intentions, which is by far not incidental but results from Lull’s mystical attitude, disproves the claim that Lull’s Art, as a theory of argument, was a medieval anticipation of the program of algorithmization or mechanization of proofs. Even if there were such anticipations of formalistic approach to reasoning in Lull’s times, they are to be sought (as shown above) in the nominalist, i.e., terminist trends, and not in his intuitionistic doctrine.

A promising way to gain them starts from reading Leibniz’s dissertation De arte Combinatoria in which Lull’s project appears in a very wide spectre of similar enterprises, all of them grown in the same culture of syllogistic; in that culture the reduction of logic to certain subject-predicate forms created the illusion of the decidability of all problems through exhaustive combinations of subjects and predicates.

Thus the said title proves symptomatic of that philosophy of knowledge, and of the resulting research programmes, which started with Aristotle, and did not entirely disappear until the scientific revolutions of our own century (quanta, relativity, evolutionistic cosmology, Gödel limitative theorems). When taking advantage of the cosmological concept of stationary universe (as opposed to the evolving universe), and linking it with the familiar logical idea of the universe of discourse (as the totality of the things under study i.e., those represented by variables), we can introduce the analogical pair of notions: that of the stationary universe of discourse and that of the evolving universe of discourse.

The combinatorial approach is associated with the vision of the stationary universe of discourse, in which the set of concepts forming our knowledge, and thus constituting the universe of discourse, is definitely fixed and closed. Then the whole cognitive enterprise would consist in searching for those combinations of members of the universe which yield true propositions, while logician’s task would involve providing people with a proper method of doing that. It was just the programme of the famous logica inventionis as mentioned in the subtitle of De arte combinatoria. The combinatorial approach, though, turned out quite sterile as far as the development of knowledge is concerned, since the universe of discourse of scientific theories rapidly expands as ever new concepts appear, and that would require ever new recombinations.

On the other hand, the combinatory strategy is fairly suitable in the problem-solving as carried out by computers. It can be seen in the mechanical checking of a proof, where each formula has to be resolved into its atomic components being combinatorily surveyed in order to detect inconsistencies among them, if there happen to be any.

The author whom Leibniz praised most is Thomas Hobbes to whom he owed the idea that every mental operation consists in computing.

The author divides the components of analysis of Lull’s merit to the claim of being a pioneer to computation into: combinatorial and formalistic. It is said that while formalistic is to be given more significance in assessing Lull’s role (by which Lull hasn’t anything much novel), Lull’s combinatorial aspect should not be entirely disregarded.

Nevertheless, the combinatorial aspect should not be entirely disregarded. It played a crucial role in the erroneous belief in the stationary universe of discourse, i.e., that human knowledge can be definitely and safely established by combinatorial procedures. That human brains were mistaken for computers in those ages in which nobody dreamt of the latter, it was one of those errors which paved the way to new understandings.

* Chapter 3
Leibniz’s Idea of Mechanical Reasoning at the Historical Background

** 3.1 An interaction between logic and mathematics

Since Aristotle up to our times, and specially in the 17th century, logic was addressed to human beings to improve their intellectual performances. Those intentions notwithstanding, the most important final result of that process, which started with the rise of logic, consists in the invention of reasoning machines. That human minds can do without a logical theory, when relying on their inborn logical capabilities alone, is obvious when one observes the history of discoveries and other manifestations of creative thinking. But the mechanical mind’s inference are due to some devices for which a logical theory forms an indispensable foundation.

The possibility of mechanical reasoning depends on the existence of what we call the logical form of a linguistic expression, that is to say, a structure determined by those terms which are relevant to logical validity. When a reasoning is so formulated that its validity can be recognized on the basis of its form alone, we call it a formalized reasoning. It may be called mechanized as well, but in a broader sense, namely that a mechanical procedure, i.e., one that does not appeal to our understanding of the content, is sufficient to judge the validity.

When we take the term ‘mechanized’ in a strict meaning, then formalization is a preparatory step towards mechanization. The latter consists, so to speak, in expressing the logical form in a language of physical states subjected to some causal laws of physics, be it mechanics (as, e.g., in Babbage’s machine), be it the domain of electronics (as in modern machines). Thus hte logical operations is identified with physical processes occurring in a machine, and this, let us repeat, yields the mechanization of reasonings in the strict meaning. This explaining of the role of formalization of reasonings as crucial for their mechanization should make obvious why so great import is attached to the former in the story being told in this volume.

The merit of founding logic goes back to Plato and Aristotle, the former as the one who discovered logical validity, the latter as the discoverer of how that validity depends on logical form.

As for Plato’s contribution, the core of Socratic method of argument, as presented in Plato’s dialogues, consists in discerning between the truth of a statement and its being deduced with a valid inference. In a typical argumental dialogue, Socrates helps his interlocutor to deduce some consequences forme a view which the partner claims to be true. When, nevertheless, the consequence proves falso, the partner is bound to acknowledge the falsity of his initial view; this reveals how the validity of an argument can be independent of the truth of its premises. Such is the constant strategy of Socrates, though no attempt is made to theoretically justify that practice.

To justify it, one should have resorted to the form of the sentences involved in an argument. This was done by Aristotle by introducing letters to represent variable contents (subjects and predicates) while the form, rendered by expressions as ‘every … is’, ’‘is not’, etc. remains constant. Though originally there was no direct translation of that form into physical states of a machine, the later development from syllogistic to Boole’s algebra of classes, and then the transforming of the latter into Frege’s algebra of truth functions, and next the invention of the method (due to Claude Shannon) to represent truth-functions by some states of an electrical device, has led to the nowadays mechanization of reasoning.

In spite of its appearance of mechanical (because of the rotating wheels) Lull’s art was no step toward mechanization of arguments, for it did not involve any thought of mathematical operations. It was Hobbes who perceived an analogy between reasoning and computing, and then it was Leibniz who enthusiastically endorsed the idea and contributed to its accomplishment in an algebraic manner. This is why Leibniz so appreciated the idea of logical form as found in some medieval students of syllogistic, as akin to the form of algebraic calculations.

There was methodological feedback between logic and geometry consisted in the fact that the ideas worked out in logical theory used to pass to the praxis of geometricians, who in turn provided logicians with the best patterns of proofs. Aristotle, when writing his Analytics, drew from the interpretations of geometry that were known to him the pattern of necessary knowledge and reliable inference, while Euclid, when writing his Elements half a century later, availed himself of the methodological concept of common axioms that is, those that are not specify, say, to geometry, but are drawn from some more general theory; note in this connection that the Aristotelian example of such an axiom — “if equals be taken from equals the remainders are equal” – occurs on the list of axioms in the first book of Euclid’s Elements.

When arithmetic and algebra, born in Babylonia, met in the Hellenistic period with Greek geometry and logic, they gave rise to that gigantic ‘tree of knowledge’ of which our civilization is the fruit. Arab scholars made great achievements in the development of algebra, continued by European scholars.

The realization of the breakthrough manifested itself in the birth of the term analytica speciosa to single out mathematics using such notation that one symbol does not correspond to a single object but to a class or species of some numbers. It was Descartes who became the coryphaeus of that analytics when in 1637 he published his Geometry, to which Discours de la méthode was the annex; in this work he gave a synthesis of geometry with algebra or analytics.

Owing to the maximal generality which the notation using letters gave to algebra, Leibniz was in a position to realize that al etter need not refer to a class of numbers, but can refer to any class of objects of any kind. The use of letters was invented for logic already by Aristotle, but only the successes of algebra could give birth to the idea of an algebraic treatment. That was one of the greatest of Leibniz’s ideas concerned with logic. It was partly materialized by himself, but it was first published in print two hundred years later, after the same discovery had been made in he meantime by other authors.

The permanent imprint of algebra upon the mentality of people living in the 17th and 18th century consisted in their experience of  how an appropriate language renders thinking more efficient and signally contributes to the solution of problems. This fact played its role in the development of a movement for improving the whole language of science.

The second discovery of the algebra of logic took place in England in the mid-19th century, and was due to a Pleiad of prominent algebraicians, of whom George Boole rendered the greatest services to logic. He was helped in that respect by the then nascent comprehension of the abstract nature of algebra, which is to say that an algebraic theory does not refer to any specified domain (which was particularly emphasized by George Peacock). Algebra can, find application or, more precisely interpretation, in a class of structurally similar domains that can be described with the means provided by a given algebraic theory. In this way algebra, after having developed from the old science of solving equations, has become the most general theory of structures, that is systems of objects for which certain operations are defined. Such operations are described from the point of view of their properties, e.g., commutativity or its lack or the performability of operations with a neutral element (such as zero for addition in a certain algebra having an interpretation in the arithmetic of natural numbers), and the like.

One of the interpretations of this calculus called Boolean algebra corresponds to that part of logic which is known as the truth-functional calculus. The same algebra has another interpretation in tradition syllogistic, and it was just that interpretation which was so penetratingly anticipated by Leibniz.

Boolean algebra can be interpreted arithmetically, the domain of natural numbers being limited to zero and unity. It can also be interpreted in many other ways, but two interpretations, one in the domain of logical sentences, and the other in that of sets, are fundamental for logic.

In the domain of sentences, multiplication is interpreted as the linking of sentences by conjunction (and); complement is interpreted as the negation of a given sentence: 1 as truth, 0 as falsehood. If a formula consisting of those symbols and variable symbols standing for sentences is always true, that is, if it is true regardless of whether truth or falsehood is assigned to the variables which occur in that formula, then it is a law of logic and belongs to the logical theory termed truth-functional, or sentential, calculus.

When interpreting Boolean algebra in the domain of sets (which traditionally were referred to as extensions of names) we can in a natural way express the four traditional kinds of categorical sentences, which form the building material of syllogisms.

Every A is B: A * not B = 0
No A is B: A * B = 0
Some A is B: A * (B + 0) ? or A * not B notEqual 0

Such a translation of traditional logic into Boolean algebra enables us to activate a strong deductive apparatus of algebra for obtaining economical and elegant proofs of the theorems of traditional logic.

The anticipation of that interpretation, to be found, in Leibniz’s treatise entitled Generates inquisitiones de analyst notinum et veritatum (inquiry into a general theory of concepts and judgements), differs from the modern form mainly by the fact that in place of the symbols notEqual 0 it has the Latin phrase est ens i.e., ‘is an entity’ (or ‘exists’), while the symbols = 0 have the analogue in ‘non est ens’ i.e., ‘is not an entity’ (or ‘does not exist’). These phrases can be interpreted in at least 2 ways: as stating the existence of objects which are the extension of a certain concept, i.e., a certain set (extensional interpretation), or as stating the existence or non-existence of combinations of properties, i.e., the intension of a certain concept (intensional interpretation). Leibniz himself was open to both interpretations; he valued the extensional one as technically effficient, but believed the intensional one to be better because of certain philosophical considerations. The idea of the translation of a sentence consisting of subject, copula, and predicate, such as “every man is intelligent” into an existential sentence of the kind “non-intelligent manhood does not exist”, was of scholastic provenance, to which Leibniz clearly referred. The brilliance of his own idea consisted in noticing an analogy between such sentences and equations of the algebra he had formulated (which included the operation of linking concepts and negating a concept). In those equations on the one side we find such a combination of concepts (one of which may be negated), and on the other, one of the two ‘magnitudes’ expressed by the terms ‘ens’ and ‘non-ens’.


The founders of the algebra of logic, that is G. W. Leibniz and G. Boole, and also A. De Morgan, E. Schröder, C. S. Peirce and others were convinced that the whole of logic can be contained in algebraic calculus. They were also aware of the fact that traditional logic, even after its algebraic reconstruction, lacked the means required to describe relations (for instance, it was impossible to render in its language even such a simple relational sentence as “for every number there is a number greater than it”). That was why the next stage was to consist in adding the algebra of relations to the already existing algebra of sets (i.e., the analogue of the traditional theory). These new researches, carried out mainly be De Morgan, Schröder, and Peirce, gave rise tot he theory of relations, which became an important and indispensable discipline in the border area between logic and set theory, but its language (combined with that of the algebra of sets) did not suffice to express mathematics in its entirety, either.

The 19th century, regardless of the intentions of the various authors, witnessed the need, and at the same time the possibility, of constructing a universal symbolic language of mathematics in which well defined symbols and precise syntactic rules would replace the vocabulary and syntax drawn from natural language.

At the turn of the 19th century, logic entered a new path by providing mathematics with a universal and rigorous symbolic language, which had also far-reaching (though not universal) applications in other disciplines and in everyday discourse. For that to happen the 17th century program of a universal language had to be revived, which did take place owing to the publication for the first time, in 1840 by J. E. Erdmann of some logical writings of Leibniz which formulated the program. There were three founders of contemporary logic, independent of one another when it comes to ideas even though they had intensive contacts with one another: Gottlob Frege, Giuseppe Peano, and Bertrand Russell. All of them referred to Leibniz’s ideas, and two of them believed themselves outright to be the executors of his testament by carrying out his programme of universal language.

Frege in 1879 published a work which presented the whole of contemporary logic (i.e., the sentential calculus and the predicate calculus) under the title Begriffsschrift, meaning conceptual writing, and can in Latin be pointedly rendered by Leibniz’s term ‘characteristica rationis’.

When new logical calculi developed at the turn of the 19th century, namely the predicate calculus (which overcame the limitations of algebra) and the sentential calculus (which may be treated as a certain interpretation of Boolean algebra), new vistas were opened to the logicians. On the one hand, it was now possible to develop and improve the calculi themselves by formulating ever new versions and by constructing other calculi themselves by formulating ever new versions and by constructing other calculi that could be superstructed upon the former (such as modal logics). On the other, since those calculi abounded in problems to be investigated, it was also possible to pose questions about the consistency of each of them, their completeness, the purposes they could serve, the relations among the axioms and concepts of a given system (the problem of independence), and finally the relations among the various systems (the interpretability of one of them in terms of another, the consistency of one of them on the assumption of the consistency of another, etc.). Precisely the same questions can be posed about mathematical theories. Among these, the arithmetic of natural numbers is of particular importance from the logical point of view, because the remaining branches of mathematics can in a way be reduced to arithmetic.

Thus, when at the world congress of mathematicians in 1900, David Hilbert presented the mathematical community the task of proving the consistence of mathematics (in view of the antinomies discovered at that time), it was known that it would suffice to concentrate on the problem of the consistency of the arithmetic of natural numbers using for that purpose the logical apparatus, both that which was already known at that time and that which was still to be created. We owe to Hilbert the term ‘metamathematics’ to denote such studies. They were carried out intensively in the period 1920s – 1930s and brought out astonishing results due to Hilbert’s companions as well as critics such as Herbrand, Gentzen, Turing, Gödel, and Tarski.

As metamathematics developed, it was more and more penetrated by mathematical concepts and methods as indispensable instruments of research. They were in particular drawn from arithmetic (e.g., the use of recursive functions initiated by Gödel), algebra (e.g., Tarski’s algebraic approach to non-classical logics), topology and set theory.

*** 3.2 The Renaissance reformism and intuitionism in logic

The development of all ideas, and hence also that of logic in the 17th century, is immersed in the melting pot of general civilizational development, in the cultural ferment of the period. The century which will be described here brings out with particular clarity the fact that the masterpieces of intellect grow from the roots of tradition but also from a lively dialogue with the milieu that is contemporaneous with their authors. And that milieu means not only congenial individuals, but also the audience consisting of readers, disciples, opponents, snobs etc., in a word, the entire enlightened public.

During the 17th century, the demarcation line between scientists and craftsmen was liquid at that time, which is illustrated by the large number of craftsmen in the London royal Society, the greatest collective authority in science of the period.

There was one more heritage of the Renaissance from which the 17th century benefited, namely Pythagorean and Platonic philosophy, from its earliest beginnings most closely intertwined with mathematics. The Platonic trend was always present in European thought, especially from the time when it was reinforced by the idea of Plotinus and in that new form, called Neo-Platonism by historians, etsablished contacts with young and vigorous Christendom, in both its orthodox version and that marked by the influence of gnosis. A significant example of that synthesis can be seen in the person of Proclus of Constantinople, one of the widest known commentators of Euclid and a Platonizing theologian, to who ideas Kepler used to refer. The first Christian writers and Church fathers were as a rule Neo-Platonians. This applies also to the greatest of them, St. Augustine of Hippo, whose ideas were truly reincarnated in the 17th century in the work of Descartes, Pascal, and the milieu of Port Royal, and in the late 19th century came to the rescue of Georg Cantor, the authority of the theory of infinite sets, in his clash with the philosophy of mathematics that followed the Aristotelian approach to infinity.

Marsilio Ficino, Nicolaus, of Cusa, Leonardo da Vinci, Nicolaus Copernicus, Galileo Galilei, Johannes Kepler all endorsed Platonic vision of the world.

In the 17th century two vast currents, one of them linked to technology and economics and ideologically based on the catchword ‘the kingdom of man’, and the other permeated by Platonic metaphysical speculation. Both influenced logic when postulated that logic should give the human mind an instrument whereby it could arrive at the truth. While they had one and the same goal in view they disagreed as to the method of reaching the truth: the former staked on induction, whereas the latter, preoccupied iwth mathematics in the Platonic manner, posutlated a purely deductive method (more geometrico).

